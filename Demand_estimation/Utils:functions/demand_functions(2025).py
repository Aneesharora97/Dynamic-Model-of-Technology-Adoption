# -*- coding: utf-8 -*-
"""All functions(demand Estimation).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17YsxYB1u7NvKSmIAEUMmYl3snAcxgm9V
"""

import numpy as np
import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
import matplotlib.pyplot as plt
import scipy.stats as stats
import seaborn as sns
import scipy
from scipy.optimize import minimize
import time
import multiprocessing as mp
import pickle
from sklearn.linear_model import LinearRegression
from numba import jit, njit, prange
# !pip install miceforest
import scipy.special
from scipy.special import logsumexp
import pdb
# !pip install pyblp
# import pyblp

def normalising_exchange(df, cols):
    # Normalize each column in cols by dividing by the value in the first row of that column
    for col in cols:
        new_col_name = f'{col}_n'
        df[new_col_name] = df[col] / df.loc[0, col]

# normalising_exchange(df, ['Japan', 'Korea', 'China'])

#generates heat map given corr matrix of a df:

def corr_heat_map(corr_matrix):
  plt.figure(figsize=(8, 6))  # Adjust the size of the figure if necessary
  sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)

  # Show the plot
  plt.title("Correlation Heatmap of Selected Features")
  plt.show()

#lets define a function for generating instruments. these are made by Aneesh and it seems wrong and stupid. Try not using them

def generate_instruments(df, column_names):
    inst_column_names=[]
    for column_name in column_names:

        #Function 1: this will generate instruments for the specific characetrtics such that the
        #instruments are the sum of charctristics of all firms(including the other products of given firms) in a given market
        # which means we include the products of its own firm)

        df[f'Ins_1_{column_name}'] = df.groupby('Quarter')[column_name].transform(
            lambda x: (x.sum() - x) / (x.count() - 1))
        inst_column_names.append(f'Ins_1_{column_name}')

        #Function 2: this will generate instruments sucha that instruments are over
        #product characteristics of all other products sold by the same firm in the given market

        df[f'Ins_2_{column_name}'] = df.groupby(['Quarter', 'Brand'])[column_name].transform(
            lambda x: (x.sum() - x) / (x.count() - 1))
        inst_column_names.append(f'Ins_2_{column_name}')

        #Function 3: this will generate instruments sucha that instruments are over product
        #characteristics of all other products sold by the different firms in the given market
        N1 = df.groupby('Quarter')[column_name].transform('sum')
        N2 = df.groupby(['Quarter', 'Brand'])[column_name].transform('sum')
        D1 = df.groupby('Quarter')[column_name].transform('count')
        D2 = df.groupby(['Quarter', 'Brand'])[column_name].transform('count')
        instr = (N1 - N2) / (D1 - D2)
        df[f'Ins_3_{column_name}'] = instr
        inst_column_names.append(f'Ins_3_{column_name}')

    return inst_column_names

#Creatign instrument again:
#1) may also check by lagging the exchange rates
#2) can also drop exchangr ates ands see rreuslts
#3) drop some columns of instruments and check


#Creating fan and yang instruments. This was my first attempt. The function seems correct but just does not account for products with age >0 .
#Check the fan_yang_inst_1 functiopn below for that

import pyblp
import numpy as np
import pandas as pd

def prepare_pyblp_instruments(df, inst_cols, rename_dict=None, formulation_str=None, diff_version='local', diff_interact=False):
    """
    Prepares product data and builds BLP-style and differentiation instruments for pyblp.

    Parameters:
    -----------
    df : DataFrame
        Original data
    inst_cols : list
        List of product characteristic columns used as instruments
    rename_dict : dict
        Dictionary mapping original column names to pyblp-friendly names
    formulation_str : str
        pyblp.Formulation string (e.g., 'Screen + Storage + Camera + Processor')
    diff_version : str
        Version of differentiation instruments ('local' or 'global')
    diff_interact : bool
        Whether to use interaction terms in differentiation instruments

    Returns:
    --------
    df_pyblp_inst : DataFrame
        Cleaned and renamed DataFrame ready for pyblp
    pyblp_inst : ndarray
        BLP-style instruments
    pyblp_inst_diff : ndarray
        Differentiation instruments
    """

    # Default rename dict if not provided
    if rename_dict is None:
        rename_dict = {
            'Screen Size': 'Screen',
            'Storage (GB)': 'Storage',
            'Camera': 'Camera',
            'Processor Speed': 'Processor',
            'Brand_Id': 'firm_ids',
            'Price': 'prices',
            'P_share': 'shares',
            'Quarter_q': 'market_ids'
        }

    if formulation_str is None:
        raise ValueError("You must provide a valid formulation string (e.g., 'Screen + Storage + Camera + Processor')")

    # Prepare product data
    cols_needed = inst_cols + ['Quarter_q', 'Brand_Id']
    df_pyblp_inst = df[cols_needed].rename(columns=rename_dict)

    # Create pyblp formulation
    formulation = pyblp.Formulation(formulation_str)

    # Build instruments
    pyblp_inst = pyblp.build_blp_instruments(formulation, product_data=df_pyblp_inst)
    pyblp_inst_diff = pyblp.build_differentiation_instruments(
        formulation,
        product_data=df_pyblp_inst,
        version="local",
        interact=False
    )

    # Diagnostics
    # print("pyblp product data shape:", df_pyblp_inst.shape)
    # print("Condition number of BLP instruments:", np.linalg.cond(pyblp_inst))

    return df_pyblp_inst, pyblp_inst, pyblp_inst_diff


def fan_yang_inst(df,columns_list):
    # we will include the following charcteristcis in our IV calculation
  #[Cons, Screen Size ,Storage (GB) ,Camera, Processor Speed] # can also add tehc dummies if i want:
  #We will create the following instruments:
  #-Sum over pre existing products that 1) are of the same brand 2) are currently in the market
  #-Sum of pre existing products that 1) are of different brand 2) currently in the amrket
  #Numnbebr of columns will be equivalent to number of characeterrsitcis*2 ( since we have two types of instruments) and number of products i.e j
    df_inst=pd.DataFrame()
    df_inst=df[["Brand","Quarter"]+ columns_list].copy()

    for col in columns_list:
        #set of instruments for same brands
        df_inst['Sum']=df_inst.groupby(['Brand', 'Quarter'])[col].transform('sum')
        df_inst[f'Instrument 1_{col}'] = df_inst['Sum'] - df_inst[col]

        #set of instruments for different brands:
        # Calculate the total sum of 'col' for each 'Time Trend' group (including all brands)
        total_sum_by_time_trend = df_inst.groupby('Quarter')[col].transform('sum')

        # Calculate the sum for each 'Brand' and 'Time Trend' combination (including the product's own brand)
        brand_sum_by_time_trend = df_inst.groupby(['Brand', 'Quarter'])[col].transform('sum')

        # Calculate the sum for other brands within the same 'Time Trend' by subtracting the product's own brand's sum
        df_inst[f'Instrument 2_{col}'] = total_sum_by_time_trend - brand_sum_by_time_trend





      #return only the instrument matrix
    return df_inst.loc[:, ~df_inst.columns.isin(['Brand', 'Quarter','Sum']+columns_list)]

#These ones are again written by me and seems closes to fan adn yangs smartphoen paper

def fan_yang_inst_1(df, columns_list):
    # Initialize the instrument dataframe and copy relevant columns (keep original size)
    df_inst = df[["Brand", "Time Trend", "Age"] + columns_list].copy()


    # For each characteristic column, calculate the instruments:
    for col in columns_list:

        #sum groupds with same time and brand. only sum values with age >0
        same_group_sum=df_inst.groupby(['Brand','Time Trend'])[col].transform(lambda x: x[df_inst['Age']>0].sum())
        #get values of individual characteristics only wherer age >0. this value must be subtracted from the sum to ensure we subtract own product characteristics
        chara_age= df_inst[col][df_inst['Age'] > 0].reindex(df_inst.index, fill_value=0)
        #The reindex ensures that the result aligns with the original index of data1, filling in missing values (for rows where 'Age' <= 0) with 0.



        # Exclude the current product from the sum by subtracting the current product's value
        df_inst[f'Instrument 1_{col}'] = same_group_sum - chara_age

        # Instrument 2: Other Brands (excluding the current product's brand)
        # Calculate the total sum of 'col' for each Time Trend group (Age > 0, including all brands)
        total_sum_by_time_trend = df_inst.groupby('Time Trend')[col].transform('sum')

        # Calculate the sum of the current brand's products within the same Time Trend
        brand_sum_by_time_trend = df_inst.groupby(['Brand', 'Time Trend'])[col].transform('sum')

        # The sum of products from other brands in the same Time Trend
        df_inst[f'Instrument 2_{col}'] = total_sum_by_time_trend - brand_sum_by_time_trend

    return df_inst.loc[:, ~df_inst.columns.isin(['Brand', 'Time Trend','Age']+columns_list)]

####The folowing was written on june 7 2025 with some improvement to above comented function####
# def fan_yang_inst_11(df, columns_list):
#     # Initialize the instrument dataframe and copy relevant columns (keep original size)
#     df_inst = df[["Brand", "Time Trend", "Age"] + columns_list].copy()


#     # For each characteristic column, calculate the instruments:
#     for col in columns_list:

#         #sum groupds with same time and brand. only sum values with age >0
#         #(written on jun 7 2025)
#         mask_age=df_inst['Age']>0

#         df_age_filtered=df_inst[mask_age]
#         same_group_sum = df_inst.merge(df_age_filtered.groupby(['Brand','Time Trend'])[col].sum().rename('group_sum'),on=['Brand','Time Trend'],how='left')['group_sum'].fillna(0)
       
#         chara_age = df_inst[col].where(mask_age, 0)
#         df_inst[f'Instrument 1_{col}'] = same_group_sum - chara_age

#        # same_group_sum=df_inst.groupby(['Brand','Time Trend'])[col].transform(lambda x: x[df_inst['Age']>0].sum())
#         #get values of individual characteristics only wherer age >0. this value must be subtracted from the sum to ensure we subtract own product characteristics
#         #chara_age= df_inst[col][df_inst['Age'] > 0].reindex(df_inst.index, fill_value=0)
#         #The reindex ensures that the result aligns with the original index of data1, filling in missing values (for rows where 'Age' <= 0) with 0.


#         #instrumen 2  (jun 7 , 2025)

#         # For Age > 0
#         df_age_filtered = df_inst[df_inst['Age'] > 0]

#         total_sum_by_time_trend = df_inst.merge(df_age_filtered.groupby('Time Trend')[col].sum().rename('total_sum'), on='Time Trend', how='left')['total_sum'].fillna(0)

#         brand_sum_by_time_trend = df_inst.merge(df_age_filtered.groupby(['Brand', 'Time Trend'])[col].sum().rename('brand_sum'), on=['Brand', 'Time Trend'], how='left' )['brand_sum'].fillna(0)

#         df_inst[f'Instrument 2_{col}'] = total_sum_by_time_trend - brand_sum_by_time_trend




#         # Exclude the current product from the sum by subtracting the current product's value
#         #df_inst[f'Instrument 1_{col}'] = same_group_sum - chara_age

#         # Instrument 2: Other Brands (excluding the current product's brand)
#         # # Calculate the total sum of 'col' for each Time Trend group (Age > 0, including all brands)
#         # total_sum_by_time_trend = df_inst.groupby('Time Trend')[col].transform('sum')

#         # # Calculate the sum of the current brand's products within the same Time Trend
#         # brand_sum_by_time_trend = df_inst.groupby(['Brand', 'Time Trend'])[col].transform('sum')

#         # # The sum of products from other brands in the same Time Trend
#         # df_inst[f'Instrument 2_{col}'] = total_sum_by_time_trend - brand_sum_by_time_trend

#     # return df_inst.loc[:, ~df_inst.columns.isin(['Brand', 'Time Trend','Age']+columns_list)]
#     return df_inst.filter(regex=r'^Instrument')


def matlab_style_instruments(df, x_excludediv_cols):
    """
    df: pandas DataFrame with columns 'Brand', 'Time', 'Age', and features in x_excludediv_cols
    x_excludediv_cols: list of column names to be used for instruments
    """
    n_obs = len(df)
    
    iv_same_oem_old = np.zeros((n_obs, len(x_excludediv_cols)))
    iv_diff_oem_old = np.zeros((n_obs, len(x_excludediv_cols)))

    age = df['Age'].values
    time = df['Time Trend'].values
    brand = df['Brand'].values
    x_excludediv = df[x_excludediv_cols].values

    for i in range(n_obs):
        ind_preexisting = age > 0
        ind_current = time == time[i]
        ind_same_oem = brand == brand[i]

        ind1 = ind_same_oem & ind_current
        ind2 = ~ind_same_oem & ind_current

        ind11 = ind1 & ind_preexisting
        ind22 = ind2 & ind_preexisting

        ind11[i] = False  # Exclude the current product

        if not np.any(ind11):
            iv_same_oem_old[i, :] = 0
        else:
            iv_same_oem_old[i, :] = np.sum(x_excludediv[ind11, :], axis=0)

        if not np.any(ind22):
            iv_diff_oem_old[i, :] = 0
        else:
            iv_diff_oem_old[i, :] = np.sum(x_excludediv[ind22, :], axis=0)

    # Convert to DataFrame for clarity
    iv_same_df = pd.DataFrame(iv_same_oem_old, columns=[f"IV_same_{col}" for col in x_excludediv_cols])
    iv_diff_df = pd.DataFrame(iv_diff_oem_old, columns=[f"IV_diff_{col}" for col in x_excludediv_cols])
    
    return pd.concat([iv_same_df, iv_diff_df], axis=1)

from scipy.stats import zscore

# def get_z_score(df, cols):
#     # Create a copy of the original columns for storing Z-scores
#     df_z_score = df[cols].copy()

#     # List to store the names of the z-score columns
#     z_score_cols = []

#     # Loop through each column and compute its Z-score
#     for col in cols:
#         # Compute Z-scores and add them as new columns in the original DataFrame
#         df[f'Z_score_{col}'] = zscore(df[col])

#         # Store the Z-score column name in the list for later analysis
#         z_score_cols.append(f'Z_score_{col}')

#         # Store the Z-scores in df_z_score for further analysis
#         df_z_score[f'Z_score_{col}'] = zscore(df[col])

#     # Return the list of Z-score column names and the DataFrame with Z-scores
#     return z_score_cols, df_z_score

def get_z_score(df, cols):
    # Create a new DataFrame with just the selected columns
    df_z_score = df[cols].copy()

    # List to store the names of the Z-score columns
    z_score_cols = []

    for col in cols:
        z_col_name = f'Z_score_{col}'
        df_z_score[z_col_name] = zscore(df[col], nan_policy='omit')
        z_score_cols.append(z_col_name)

    return z_score_cols, df_z_score


#define a function that takes the column and gives yout eh graph for avergae over quarters. This should tell us something about the scaling we need to do
def avg_graph(df, column_names, show_plot=False):
    for item in column_names:
        gg = df.groupby("Quarter")[item].mean().reset_index()
        plt.plot(gg['Quarter'], gg[item], label=item)
    plt.xlabel('Quarter')
    plt.ylabel('Average Value')
    plt.legend()
    plt.tight_layout()
    if show_plot:
        plt.show()
    else:
        plt.close()  

from sklearn.preprocessing import MinMaxScaler

def min_max_scale_df(df, characteristics_columns):
    """
    Scales specified columns in a DataFrame using MinMaxScaler from scikit-learn.

    Parameters:
    df (pd.DataFrame): Input DataFrame.
    characteristics_columns (list): List of column names to scale.

    Returns:
    pd.DataFrame: DataFrame with scaled characteristic columns.
    """
    df_scaled = df.copy()
    scaler = MinMaxScaler()
    df_scaled[characteristics_columns] = scaler.fit_transform(df[characteristics_columns])
    return df_scaled

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import pandas as pd
import matplotlib.pyplot as plt

def run_pca_analysis(df, characteristics_columns, n_components=3, plot=True):
    """
    Runs PCA on selected columns of a DataFrame, returns PCA-transformed data,
    loadings, and optionally displays a biplot.

    Parameters:
    df (pd.DataFrame): Input DataFrame.
    characteristics_columns (list): Columns to include in PCA.
    n_components (int): Number of principal components to compute.
    plot (bool): Whether to plot PC1 vs. PC2 with loadings.

    Returns:
    pca_df (pd.DataFrame): DataFrame of principal components.
    loadings (pd.DataFrame): PCA loadings for each feature.
    explained_variance (np.array): Explained variance ratio for each component.
    """
    # Standardize features
    X = df[characteristics_columns]
    X_scaled = StandardScaler().fit_transform(X)

    # Fit PCA
    pca = PCA(n_components=n_components)
    principal_components = pca.fit_transform(X_scaled)

    # Create DataFrame for components
    pc_labels = [f'PC{i+1}' for i in range(n_components)]
    pca_df = pd.DataFrame(data=principal_components, columns=pc_labels)

    # Create loadings matrix
    loadings = pd.DataFrame(
        pca.components_.T,
        columns=pc_labels,
        index=characteristics_columns
    )

    # Print explained variance
    explained_variance = pca.explained_variance_ratio_
    print("Explained Variance Ratio:", explained_variance)

    # Plot PC1 vs PC2 with loadings arrows
    if plot and n_components >= 2:
        plt.figure(figsize=(10, 6))
        plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.5)

        for i, feature in enumerate(characteristics_columns):
            plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], 
                      color='r', alpha=0.5)
            plt.text(pca.components_[0, i]*1.2, pca.components_[1, i]*1.2, 
                     feature, color='darkred')

        plt.xlabel('PC1')
        plt.ylabel('PC2')
        plt.grid()
        plt.title('PCA Biplot (PC1 vs PC2)')
        plt.show()

    return pca_df, loadings, explained_variance   




from statsmodels.stats.outliers_influence import variance_inflation_factor
def compute_vif(X, var_names=None):
    """
    Computes VIF for each variable in the instrument matrix X.

    Parameters:
    - X: 2D numpy array or pandas DataFrame (instrument matrix)
    - var_names: optional list of variable names

    Returns:
    - DataFrame with variable names and their VIF scores
    """
    if isinstance(X, np.ndarray):
        X = pd.DataFrame(X, columns=var_names if var_names else [f'X{i}' for i in range(X.shape[1])])

    vif_data = pd.DataFrame()
    vif_data["Variable"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return vif_data

import pandas as pd


from sklearn.ensemble import IsolationForest
from scipy.spatial.distance import mahalanobis
from scipy.stats import chi2
import numpy as np

def get_isolation_forest_outliers(df, columns, contamination=0.01):
    """
    Returns: array of outlier flags (1 for inlier, -1 for outlier)
    """
    X = df[columns].copy()
    iso = IsolationForest(contamination=contamination, random_state=42)
    return iso.fit_predict(X)


def get_mahalanobis_distances(df, columns):
    """
    Returns: Series of Mahalanobis distances for each row
    """
    X = df[columns].copy()
    cov = np.cov(X.T)
    inv_cov = np.linalg.inv(cov)
    mean_vec = X.mean(axis=0)

    distances = X.apply(lambda row: mahalanobis(row, mean_vec, inv_cov), axis=1)
    return distances


def aggregate_data(df, columns_to_aggregate, specific_brands):
    """
    Aggregates feature data by Brand and Quarter, maps non-specified brands to 'Other',
    merges max tech specialization, and adds a constant column.

    Parameters:
    -----------
    df : pd.DataFrame
        The original dataset containing product or tech specs across brands and quarters.

    columns_to_aggregate : list
        List of numeric column names to average (e.g., ['Screen Size', 'Storage (GB)', ...]).

    specific_brands : list
        List of brands to preserve. All others will be grouped under 'Other'.

    Returns:
    --------
    pd.DataFrame
        Aggregated DataFrame with mean values, max tech specialization, brand mapping, and a constant column.
    """

    # 1. Average selected features per Brand and Quarter
    df_aggregate = df.groupby(['Brand', 'Quarter'])[columns_to_aggregate].mean().reset_index()

    # 2. Convert any brand not in specific_brands to 'Other'
    df_aggregate['Brand_d'] = df_aggregate['Brand'].apply(lambda x: x if x in specific_brands else 'Other')

    # 3. Get max tech specialization per Brand and Quarter
    tech_aggregate = df.groupby(['Brand', 'Quarter'])['Spec_tech'].max().reset_index()

    # 4. Merge mean-aggregated data with tech specialization
    df_aggregate = pd.merge(df_aggregate, tech_aggregate, how='left', on=['Brand', 'Quarter'])

    # 5. Add a constant column for modeling purposes
    df_aggregate['Cons'] = 1

    return df_aggregate


from scipy.linalg import qr

def drop_collinear_columns(df, verbose=True):
    """
    Identifies and removes linearly dependent columns from a DataFrame.

    Parameters:
    ----------
    df : pd.DataFrame
        Input DataFrame to check for multicollinearity.
    
    verbose : bool
        Whether to print matrix shape, rank, and dropped columns.

    Returns:
    -------
    Tuple of:
        - cleaned_df: pd.DataFrame without linearly dependent columns
        - dropped_columns: list of dropped column names
    """
    matrix = df.to_numpy()
    _, r, p = qr(matrix, pivoting=True)
    rank = np.linalg.matrix_rank(matrix)
    dependent_cols = df.columns[p[rank:]]  # Columns after rank are dependent

    if verbose:
        print(f"Original shape: {df.shape}")
        print(f"Matrix rank: {rank}")
        print(f"Dropped columns: {list(dependent_cols)}")

    cleaned_df = df.drop(columns=dependent_cols)
    return cleaned_df, list(dependent_cols)





#clearly we need to scale processor speed and storage

@jit(parallel = True)
#Here x is the characteristics that have random coefficeints
def util_iter(out, x, v, p, delta, sigma_v, J, T, N): #D is the demographic variables vector. D*1 column vector.
#sigma_v and sigma_b are random coefficients of the dmeographic factors
   # first iterate over the individuals
    for i in prange(N): #Prange just means that this loop will porcess parallely.
        # iterator through t and j
        tj = 0
        for t in prange(T):
          # market size of market t
            mktSize = J[t]  #J is teh total number of good in market t.


#V is the unobswerved deomgraphic variables which are random normal
#sigma_v is the varaible assocaited with unobserved variable v followiubg normal distribution. think of sigma_v as varaince of V

            # iterate over goods in a particular market
            # V is the part of utility which is individula dependent

            #Please note that theta is numerically column vector. But in python, we do not really need to mention it as a row or column if the vector is 1d. pyhton will internally manipulate it.
            #We have 5 characteristics in X and hence k=5

            #for x=cons,Screen Size",Storage (GB)"'Camera',Processor Speed",price( that is why there is negartive sign begfore the last line)
            for j in prange(mktSize):
                out[tj, i] = delta[tj] + \
                  x[tj, 0] * v[N * t + i, 0] * sigma_v[0] + \
                  x[tj, 1] * v[N * t + i, 1] * sigma_v[1] + \
                  x[tj, 2] * v[N * t + i, 2] * sigma_v[2] + \
                  x[tj, 3] * v[N * t + i, 3] * sigma_v[3] - \
                  p[tj] * v[N * t + i, 4] * sigma_v[4]
                  # v[N * t + i, 1] * theta_2[1] * x[tj, 1] +
                  # v[N * t + i, 2] * theta_2[2] * x[tj, 2] + \
                  # v[N * t + i, 3] * theta_2[3] * x[tj, 3] + \
                  # v[N * t + i, 4] * theta_2[4] * x[tj, 4] - \
                  # v[N * t + i, 5] * theta_2[5] * p[tj] + \
                  # d_it[N * t + i, 0]*sigma_b[0] * x[tj,0] +\
                  # d_it[N * t + i, 1]*sigma_b[1] * x[tj,1] +\
                  # d_it[N * t + i, 2]*sigma_b[2] * x[tj,2] +\
                  # d_it[N * t + i, 3]*sigma_b[3] * x[tj,3] +\
                  # d_it[N * t + i, 0]*sigma_b[4] * x[tj,4] -\
                  # d_it[N * t + i, 0]*sigma_b[5] * p[tj]

                tj += 1

    return out

@jit

def compute_indirect_utility(x_rand, v, p, delta, sigma_v, J, T, N):
    # make sure theta_2 are positive
    sigma_v = np.abs(sigma_v)


    # output matrix
    out = np.zeros((sum(J), N)) #kind of starting guess of Out which then keeps updating in the iteration in the nexti line.

    # call the iteration function to calculate utilities
    #out = util_iter(out, x, v, p, y, delta, theta_2, J, T, N)
    out = util_iter(out, x_rand, v, p,delta, sigma_v, J, T, N)

    return out

@jit(nopython=False)
def compute_share(x_rand, v, p,delta, sigma_v, J, T, N):
    q = np.zeros((np.sum(J), N))

    # obtain vector of indirect utilities
    u =compute_indirect_utility(x_rand, v, p,delta, sigma_v, J, T, N)
    #because we have such huge values of our utilities, it is better to subtract the value of utility wight hemax value of utility across  goods for each indicidual.
    #this will not blow up the exponents. Teh folowing line takes the max of each column.
    #max_u=np.max(u,axis=0,keepdims=True) #axis 0 signifies that we run dwon along the rows. so, there will be max of each column.
    # exponentiate the utilities
    # print("indirect_utility is ",u)
    # print("-"*40)
    # print("Shape: ",u.shape)
    # print("max of utility =", np.max(u))
    # print("min of utiltiy",np.min(u))
    exp_u = np.exp(u)
    # print("exponential of utility is:",np.max(exp_u))

    # pointer to first good in the market
    first_good = 0

    for t in range(T):
        # market size of market t
        mktSize = J[t]

        # calculate the numerator of the share eq
        numer = exp_u[first_good:first_good + mktSize,:]
        # print("numer is : ",numer)
        # calculate the denom of the share eq
        #denom= 1+np.exp(logsumexp(u,axis=0))
        denom = 1 + numer.sum(axis = 0)
        #print("denom is : ",denom)


        # calculate the quantity each indv purchases of each good in each market
        q[first_good:first_good + mktSize,:] = numer/denom #q is equal to s_ijt and we take the average for the the number
        #of conumsers to get s_jt which is mentioned below as s
        #print("num/deno: ", numer/denom)
        first_good += mktSize

    # to obtain shares, assume that each simulation carries the same weight.
    # this averages each row, which is essentially computing the sahres for each
    #  good in each market.
    s = q @ (np.repeat(1/N, N)) #This simply is dividing the q computed previously with N. this gives us share.
    # print("max value of share",np.max(s))
    # print("min value of share",np.min(s))
    return (q,s) # q therefore gives us each individula quanitty and s is solving the integral through monetcarlo simulation whihcis nothing but equal to
    #taking average

#delta is solved through contraction mapping. We will later use this as an inner loop in our objective functrion.
#This function uses delta as a class object. For a simpler and direct use , check the funciton below: solve_delta_1

@jit(nopython=False)
def solve_delta(s, x_rand, v, p, delta, sigma_v ,J, T, N, tol):
    # define the tolerance variable
    eps = 10

    # renaming delta as delta^r
    delta_old = delta

    while eps > tol:
        # Aviv's step 1: obtain predicted shares and quantities
        q_s = compute_share(x_rand, v, p,delta_old, sigma_v, J, T, N)
        #q_s is the list of q( quanity purcahsed bye ach individual and s whihc are the shares)
        # extract the shares
        sigma_jt = q_s[1] #second elemtn of list q_s
        #print("predicted shares (sigma_jt) is: ",sigma_jt)
        # step 2: use contraction mapping to find delta
        # print("delta old: ",delta_old)
        # print("Long of s/sigma_jt",np.log(s/sigma_jt))
        delta_new = delta_old + np.log(s/sigma_jt)

        #print("new delta: ",delta_new)
        # update tolerance
        eps = np.max(np.abs(delta_new - delta_old))

        delta_old = delta_new.copy()

    return delta_old

#This ppart of the code avoids the usewof class object delta:
# @jit(nopython=False)
# def solve_delta_1(s, x_rand, v, p, delta, sigma_v, J, T, N, tol):
#     # Define the tolerance variable
#     eps = 10

#     # Initialize delta_old as the current delta array
#     delta_old = delta.copy()

#     while eps > tol:
#         # Aviv's step 1: obtain predicted shares and quantities
#         q_s = compute_share(x_rand, v, p, delta_old, sigma_v, J, T, N)
#         sigma_jt = q_s[1]  # Extract the shares
#         delta_new = delta_old + np.log(s / sigma_jt)  # Update delta using contraction mapping

#         # Update the tolerance and check for convergence
#         eps = np.max(np.abs(delta_new - delta_old))

#         # Update delta_old for the next iteration
#         delta_old = delta_new.copy()

#     # Return the updated delta
#     return delta_old


@jit(nopython=False)
def solve_delta_1(s, x_rand, v, p, delta, sigma_v, J, T, N, tol,max_iter=5000):
    # Define the tolerance variable
    eps = 10

    # Initialize delta_old as the current delta array
    delta_old = delta.copy()
    iterations=0

    min_share = 1e-15 # A very small positive float
    max_share = 1.0 - min_share

    while eps > tol:
        # Aviv's step 1: obtain predicted shares and quantities
        q_s = compute_share(x_rand, v, p, delta_old, sigma_v, J, T, N)
        sigma_jt = q_s[1]  # Extract the shares
        
        s_clipped = np.clip(s, min_share, max_share)
        sigma_jt_clipped = np.clip(sigma_jt, min_share, max_share)

        log_ratio = np.log(s / sigma_jt_clipped)

        # Check for NaN/Inf from log_ratio before updating delta_new
        if np.any(np.isnan(log_ratio)) or np.any(np.isinf(log_ratio)):
            # If log_ratio becomes NaN/Inf, this indicates a severe problem with the current sigma_v
            # or the shares from compute_share. Penalize this sigma_v strongly.
            # Return a large value to signal failure to the outer optimizer.
            print(f"Warning: log_ratio became NaN/Inf at iteration {iterations}. Returning penalty.")
            return np.ones_like(delta) * 1e20 # A very large penalty

        delta_new = delta_old + np.log(s / sigma_jt_clipped)  # Update delta using contraction mapping

        # Update the tolerance and check for convergence
        eps = np.max(np.abs(delta_new - delta_old))

        if np.isnan(eps):
            print(f"Warning: eps became NaN at iteration {iterations}. Returning penalty.")
            return np.ones_like(delta) * 1e20
        
        # Update delta_old for the next iteration
        delta_old = delta_new.copy()
        iterations += 1
    print("Iterations",iterations)
    if iterations == max_iter:
        # Contraction mapping did not converge within max_iter. Penalize this sigma_v.
        print(f"Warning: Contraction mapping did not converge after {max_iter} iterations. Returning penalty.")
        return np.ones_like(delta) * 1e20 # Return a large value to signal failure    
    # Return the updated delta
    return delta_old

#The objective function is nothign but the GMM objective function which is minimised for a given value.
def objective(param, s, x_rand, x_d_p_m, v, p, J, T, N, tol,
              Z, weigh):
#optimum funciton will faltten the parameters, for us to be able to use sigma_d as a amtrix we need to reshape it like this.

    sigma_v = param[0:6]

    #restricting apraemters to 0

    obs = np.sum(J)
    # print("sigma_v:", sigma_v)
    # print("sigma_d:", sigma_d)
    # Aviv's step 1 & 2:
    d.delta = solve_delta(s, x_rand, v, p, d.delta, sigma_v, J, T, N, tol) #solves value fo delta for a given guess of theta_2( the non linear parameters)
    #print("D.delta is:",d.delta)
    # print("-"*40)

    # obtain the actual implied quantities and shares from converged delta
    #q_s = compute_share(x, v, d_it,p,d.delta, sigma_v,sigma_d, J, T, N) #For given value of delta we compute share

    # calculate marginal costs
    #mc = calc_mc(q_s, firms, p, y, theta_2[5], J, T, N, marks, markets).reshape((-1,1))

    # since we are using both demand and supply side variables,
    #  we want to stack the estimated delta and estimated mc
    #y2 = np.vstack((d.delta.reshape((-1,1)), np.log(mc)))

    #We will just have Delta2 is d2:
    d2=d.delta.reshape((-1,1))
    #print("d2 is:",d2)
    #print("-"*40)

    # For now lets create a matrix that includes only demand side
  

    #lets create the matrix for instruments
    z=Z

    #settign a debugger incase of an error:
    # pdb.set_trace()

    x_d=x_d_p_m
    #here i will use print commands to do debugging
    # print("Iteration is:" ,(x_d.T @ z @ weigh @ z.T @ x_d ),"shape is:",(x_d.T @ z @ weigh @ z.T @ x_d).shape)
    # print("-"*40)



    # get linear parameters (this FOC is from Aviv's appendix) The objective function is the GMM objective function. You take the first order derivative of that function.
    b = np.linalg.inv(x_d.T @ z @ weigh @ z.T @ x_d) @ (x_d.T @ z @ weigh @ z.T @ d2)
    #print("b is:",b)
    #print("-"*40)


    # Step 3: get the error term xi  (also called omega). this is where we use the convered
    xi_w = d2 - x_d @ b

    #print("error term xi is:",xi_w)
    #print("-"*40)
    # computeo g_bar in GMM methods
    g = z.T @ xi_w

    #print('mean of g',g)
    #print("-"*40)

    obj = g.T @ weigh @ g
    
    if np.isnan(obj) or np.isinf(obj):
      obj = 1e20
    print([sigma_v, obj])


    return obj

# there is commented line for delta=solve_delta, that is the original version. use with solve delta.
# curent execution uses solve_delta_1 which avoids delta as class object


#The objective function is nothign but the GMM objective function which is minimised for a given value.
def objective_1(param, s, X_rand, X_d_p_m, v, p, J, T, N, tol,delta,
              Z, weigh):
#optimum funciton will faltten the parameters, for us to be able to use sigma_d as a amtrix we need to reshape it like this.

    sigma_v = param[0:6]

    #restricting apraemters to 0

    obs = np.sum(J)
    # print("sigma_v:", sigma_v)
    # print("sigma_d:", sigma_d)
    # Aviv's step 1 & 2:
    # d.delta = solve_delta(s, x, v, p, d.delta, sigma_v, J, T, N, tol) #solves value fo delta for a given guess of theta_2( the non linear parameters)


    delta_1=solve_delta_1(s, X_rand ,v, p, delta, sigma_v, J, T, N, tol)
    #print("D.delta is:",d.delta)
    # print("-"*40)

    # obtain the actual implied quantities and shares from converged delta
    #q_s = compute_share(x, v, d_it,p,d.delta, sigma_v,sigma_d, J, T, N) #For given value of delta we compute share

    # calculate marginal costs
    #We will just have Delta2 is d2:
    d2=delta_1.reshape((-1,1))
    #print("d2 is:",d2)
    #print("-"*40)

    # For now lets create a matrix that includes only demand side
    # x_d= x_d

    #lets create the matrix for instruments
    z=Z

    #settign a debugger incase of an error:
    # pdb.set_trace()


    #here i will use print commands to do debugging
    # print("Iteration is:" ,(x_d.T @ z @ weigh @ z.T @ x_d ),"shape is:",(x_d.T @ z @ weigh @ z.T @ x_d).shape)
    # print("-"*40)



    # get linear parameters (this FOC is from Aviv's appendix) The objective function is the GMM objective function. You take the first order derivative of that function.
    b = np.linalg.inv(X_d_p_m.T @ z @ weigh @ z.T @ X_d_p_m) @ (X_d_p_m.T @ z @ weigh @ z.T @ d2)
    #print("b is:",b)
    #print("-"*40)


    # Step 3: get the error term xi  (also called omega). this is where we use the convered
    xi_w = d2 - X_d_p_m @ b

    g = z.T @ xi_w

    #print('mean of g',g)
    #print("-"*40)

    obj = g.T @ weigh @ g
    
    if np.isnan(obj) or np.isinf(obj):
      obj = 1e20
    print([sigma_v, obj])

  


    return obj


def compute_se(xi_updated, Z, X_d_p_m, weight):
    """
    Compute robust standard errors for the linear parameters in GMM estimation.

    Parameters:
    -----------
    xi_updated : ndarray
        Vector of residuals (shape: n_obs,)
    Z : ndarray
        Matrix of instruments (shape: n_obs, n_instr)
    X_d_p_m : ndarray
        Matrix of derivatives or design matrix (shape: n_obs, n_params)
    weight : ndarray
        Weighting matrix used in GMM (shape: n_instr, n_instr)

    Returns:
    --------
    lin_std_errors : ndarray
        Robust standard errors for the linear parameters
    """

    # Moment conditions g_i = Z_i * xi_i
    moment = xi_updated.reshape((-1, 1)) * Z
    
    # Centered covariance matrix of moments
    mean_moment = moment.mean(axis=0)
    v1 = moment.T @ moment / moment.shape[0]
    v2 = np.outer(mean_moment, mean_moment)
    S = v1 - v2  # Robust covariance matrix of the moments

    # Sandwich variance formula
    ZWZ_inv = np.linalg.inv(X_d_p_m.T @ Z @ weight @ Z.T @ X_d_p_m)
    middle = X_d_p_m.T @ Z @ weight @ S @ weight @ Z.T @ X_d_p_m
    V = ZWZ_inv @ middle @ ZWZ_inv

    # Standard errors are sqrt of diagonal
    lin_std_errors_old = np.sqrt(np.diag(V))
    
    return lin_std_errors_old


def compute_full_gmm_std_errors(
    sigma_v_est,       # Current estimate of nonlinear params (sigma_v)
    beta_est,          # Estimated linear params beta (vector)
    objective_fn,      # Your GMM objective function, returns moments g_obs
    xi_updated,        # Residuals for linear part: xi = delta - X beta (vector)
    Z,                 # Instruments matrix (n_obs x n_instr)
    X_d_p_m,           # Design matrix (n_obs x n_params) for linear params
    weight,            # Weighting matrix (n_instr x n_instr)
    spacing=1e-5,      # Numerical derivative step size
    print_fn=None      # Optional function for logging (e.g. print)
):
    """
    Compute robust GMM standard errors for nonlinear (sigma_v) and linear (beta) parameters.
    """
    n_nl = len(sigma_v_est)
    n_obs, n_params = X_d_p_m.shape
    
    # Step 1: Evaluate baseline moments at current sigma_v
    g_obs0 = objective_fn(sigma_v_est).mean(axis=0)  # should return moment conditions, shape (n_instr,) or (n_obs, n_instr)
    if g_obs0.ndim == 1:
        g_obs0 = g_obs0.reshape(-1, 1)
    n_instr = g_obs0.shape[0]

    # Step 2: Numerical derivative wrt nonlinear parameters (sigma_v)
    G_nl = np.zeros((n_instr, n_nl))
    for i in range(n_nl):
        sigma_perturb = np.copy(sigma_v_est)
        sigma_perturb[i] += spacing
        
        try:
            g_obs1 = objective_fn(sigma_perturb).mean(axis=0)
            if g_obs1.ndim == 1:
                g_obs1 = g_obs1.reshape(-1,1)
            diff = (g_obs1 - g_obs0) / spacing
        except Exception as e:
            # fallback backward step
            sigma_perturb[i] = sigma_v_est[i] - spacing
            g_obs1 = objective_fn(sigma_perturb)
            if g_obs1.ndim == 1:
                g_obs1 = g_obs1.reshape(-1,1)
            diff = (g_obs0 - g_obs1) / spacing
        
        G_nl[:, i] = diff.flatten()

    # Step 3: Derivative wrt linear params (beta)
    G_beta = -Z.T @ X_d_p_m / n_obs  # shape: (n_instr, n_params)
    
    # Step 4: Combine Jacobians
    G = np.hstack([G_nl, G_beta])  # shape: (n_instr, n_nl + n_params)
    
    # Step 5: Moment covariance matrix S (robust)
    moment = xi_updated.reshape((-1,1)) * Z  # (n_obs, n_instr)
    mean_moment = moment.mean(axis=0)
    v1 = moment.T @ moment / n_obs
    v2 = np.outer(mean_moment, mean_moment)
    S = v1 - v2

    # Step 6: Sandwich variance formula
    GtWG = G.T @ weight @ G
    middle = G.T @ weight @ S @ weight @ G
    cov_matrix = np.linalg.inv(GtWG) @ middle @ np.linalg.inv(GtWG) / n_obs

    # Step 7: Extract standard errors
    stderr = np.sqrt(np.diag(cov_matrix))

    # Optional: print results
    if print_fn:
        for i in range(n_nl):
            print_fn(f"Nonlinear param sigma_v[{i}]: {sigma_v_est[i]:.6f}, stderr: {stderr[i]:.6f}")
        for i in range(n_params):
            print_fn(f"Linear param beta[{i}]: {beta_est[i]:.6f}, stderr: {stderr[n_nl+i]:.6f}")

    return stderr, cov_matrix

def Gmm_std_error(sigma_v_3, b_updated, gmm_moments, xi_updated,Z,X_d_p_m,weight,spacing=1e-5): #written by Aneesh
    g_obs= gmm_moments(sigma_v_3)
    g_obj_mean=g_obs.mean(axis=0)
    n_obs, n_moments=g_obs.shape
    k=len(sigma_v_3)
    G=np.zeros((n_moments,k))

    spacing=1e-6
    temp_sigma_v=sigma_v_3.copy()


    for i in range(k):
        sigma_v_perturbed=sigma_v_3.copy()
        sigma_v_perturbed[i]+=spacing

        try:
            g_obs1=gmm_moments(sigma_v_perturbed)
            G[:,i]= ((g_obs1 - g_obs)/spacing).mean(axis=0)
        except Exception as e:
            sigma_v_perturbed=sigma_v_3.copy()
            sigma_v_perturbed[i]-=spacing
            g_obs1=gmm_moments(sigma_v_perturbed)
            G[:,i]= ((g_obs - g_obs1)/spacing).mean(axis=0)



    G_full= np.hstack([G, (-Z.T@X_d_p_m)/n_obs]) # the second part is derivative wrt linear para

    # GMM variance-covariance formula (sandwich)
    middle = G_full.T @ weight @ G_full
    middle_inv = np.linalg.inv(middle)

    # Omega (moment covariance) approximation
    Omega_hat = (g_obs.T @ g_obs) / n_obs    # shape (n_moments, n_moments)

    var_sigma = (middle_inv @ (G_full.T @ weight @ Omega_hat @ weight @ G_full) @ middle_inv) / n_obs

    stderr_sigma = np.sqrt(np.diag(var_sigma))  # standard errors

    return stderr_sigma, G_full, G


def MC(df,alpha,q_s):
  first_model=0

  out = np.zeros((np.sum(J)))

  alpha = np.abs(alpha)

  Quarter_unique=df['Quarter_q'].unique().tolist()
  for quarters in Quarter_unique:
    model_yr=df[df['Quarter_q']==quarters]['Model_Id'].to_numpy().reshape((-1,1)) #Model IDs  for each quarter. Vasically taking all the models.
    firms=df[df['Quarter_q']==quarters]['Brand'].to_numpy().reshape((-1,1)) #brand/firm for each quarter
    num_of_models=df[df["Quarter_q"]==quarters]['Model_Id'].shape[0]
    p =df[df['Quarter_q']==quarters]['Price'].to_numpy().reshape((-1,1))
    #note that you would have multipel instances of firms for products whule each product is unique. The unique ness of the product is taken care by the "Model_Id"


    ownership=np.equal(firms,np.transpose(firms))
    prob=q_s[0][first_model:first_model + num_of_models,:] #these are individual probability for purchasing each good.

    q=q_s[1].reshape((-1,1))
    first_model += num_of_models
    dsdp = np.zeros((num_of_models, num_of_models))

    for i in range(N):
      prob_yr = prob[:,i].reshape((-1,1)) #column of the matrix which extracts all products for each individual.

      dsdp= dsdp+ (ownership * alpha* 1/N * ((prob_yr@prob_yr.T)-np.diag(prob_yr))) #This si the cross pric eelasticity matrix ( jacobian) of the same firm.

    neg_dsdp= - dsdp  #we take the netative of the cross price elasticty
    #We use this to finally get MC

    dsdp_share = np.linalg.inv(neg_dsdp) @ q

    MC =  p  -  dsdp


    out[first_model:first_model + num_of_models]=MC.flatten()

  return out



import numpy as np
import pandas as pd
import time
import datetime
import pickle
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# =============================================
# 1. Optimization Logger Class
# =============================================

class OptimizationLogger:
    def __init__(self,k=None):
        if k is None:
            raise ValueError("You must provide the number of random coefficient parameters (k)")
        """Initialize storage for all tracked metrics"""
        self.iteration_data = []
        self.best_objective = float('inf')
        self.best_params = None
        self.start_time = time.time()
        self.k=k
        self.param_names = None
        self.X_d_p_names = []
        self.b_first_stage_list = []
        
    def set_parameter_names(self, names):
        """Set names for parameters for better logging"""
        self.param_names = names
    def set_X_d_p_names(self, names):
        self.X_d_p_names = names    
        
    def log_iteration(self, iteration, sigma_v, linear_params, objective_value, 
                     residuals, time_taken, std_errors=None, delta=None,b_first_stage=None,nonlinear_std_errors=None):
        """Log all relevant data for each iteration"""
        entry = {
            'iteration': iteration,
            'sigma_v': sigma_v.copy(),
            'linear_params': linear_params.copy(),  # Second-stage b
            'b_first_stage': b_first_stage.copy() if b_first_stage is not None else None,
            'objective_value': objective_value,
            'residuals': residuals.copy(),
            'time_taken': time_taken,
            'timestamp': time.time(),
            'std_errors': std_errors.copy() if std_errors is not None else None,
            'nonlinear_std_errors': nonlinear_std_errors.copy() if nonlinear_std_errors is not None else None,
            'delta': delta.copy() if delta is not None else None,
            'linear_param_dict': dict(zip(self.X_d_p_names, linear_params)) if hasattr(self, 'X_d_p_names') else None,
            'b_first_stage_dict': dict(zip(self.X_d_p_names, b_first_stage)) if (b_first_stage is not None and hasattr(self, 'X_d_p_names')) else None
        }
        
        self.iteration_data.append(entry)
        
        # Update best solution found
        if objective_value < self.best_objective:
            self.best_objective = objective_value
            self.best_params = {
                'sigma_v': sigma_v.copy(),
                'linear_params': linear_params.copy(),
                'std_errors': np.concatenate([nonlinear_std_errors, std_errors]) if (nonlinear_std_errors is not None and std_errors is not None) else None,
                'delta': delta.copy() if delta is not None else None
}

    
    def save_progress(self, filename_prefix='optimization'):
        """Save current progress to disk"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save complete log
        log_filename = f"{filename_prefix}_log_{timestamp}.pkl"
        with open(log_filename, 'wb') as f:
            pickle.dump({
                'iteration_data': self.iteration_data,
                'best_solution': self.best_params,
                'total_time': time.time() - self.start_time,
                'param_names': self.param_names
            }, f)
        
        # Save CSV versions for easy inspection
        self._save_as_csv(filename_prefix, timestamp)

    def get_linear_param_long_format(self):
        if not hasattr(self, 'X_d_p_names'):
            raise ValueError("Parameter names not set. Call set_X_d_p_names before logging.")

        rows = []
        for entry in self.iteration_data:
            iter_id = entry['iteration']
            b1_dict = entry['b_first_stage_dict']
            b2_dict = entry['linear_param_dict']
            for var in self.X_d_p_names:
                rows.append({
                  'parameter': var,
                  'b_first_stage': b1_dict[var] if b1_dict else np.nan,
                  'b_second_stage': b2_dict[var] if b2_dict else np.nan,
                  'iteration': iter_id
              })

        df_long = pd.DataFrame(rows)
        return df_long

    def get_linear_param_long_format_with_se(self):
        if not hasattr(self, 'X_d_p_names'):
            raise ValueError("Parameter names not set. Call set_X_d_p_names before logging.")

        rows = []
        for entry in self.iteration_data:
            iter_id = entry['iteration']
            b1_dict = entry['b_first_stage_dict']
            b2_dict = entry['linear_param_dict']
            se_array = entry.get('std_errors', None)  # numpy array of std errors, aligned with X_d_p_names
            for i, var in enumerate(self.X_d_p_names):
                rows.append({
                    'parameter': var,
                    'b_first_stage': b1_dict[var] if b1_dict else np.nan,
                    'b_second_stage': b2_dict[var] if b2_dict else np.nan,
                    'std_error_second_stage': se_array[i] if se_array is not None else np.nan,
                    'iteration': iter_id
            })

        df_long = pd.DataFrame(rows)
        return df_long

    def get_nonlinear_param_long_format_with_se(self):
        if not self.param_names:\
                    raise ValueError("Parameter names not set. Call set_parameter_names before logging.")

        rows = []
        for entry in self.iteration_data:
            iter_id = entry['iteration']
            sigma_v = entry['sigma_v']
            se_array = entry.get('nonlinear_std_errors', None)
            for i, param_name in enumerate(self.param_names):
                rows.append({
                    'parameter': param_name,
                    'value': sigma_v[i],
                    'std_error': se_array[i] if se_array is not None else np.nan,
                    'iteration': iter_id
                })
        return pd.DataFrame(rows)

    def get_residuals_long_format(self):
      """
      Return residuals in long format:
      columns = ['observation', 'residual', 'iteration']
      """
      rows = []
      for entry in self.iteration_data:
          iter_id = entry['iteration']
          residuals = entry['residuals']
          for i, val in enumerate(residuals):
              rows.append({
                  'observation': i,
                  'residual': val,
                  'iteration': iter_id
              })
      return pd.DataFrame(rows)
          
    def _save_as_csv(self, prefix, timestamp):
        """Save key data as CSV files"""
        if not self.iteration_data:
            return
            
        # Save sigma_v history
        sigma_v_history = np.array([entry['sigma_v'] for entry in self.iteration_data])
        np.savetxt(f"{prefix}_sigma_v_{timestamp}.csv", sigma_v_history, delimiter=',')
        
        # Save linear parameters history
        linear_params_history = np.array([entry['linear_params'] for entry in self.iteration_data])
        np.savetxt(f"{prefix}_linear_params_{timestamp}.csv", linear_params_history, delimiter=',')
        
        # Save objective values
        objectives = np.array([[entry['objective_value'], entry['time_taken']] 
                            for entry in self.iteration_data])
        np.savetxt(f"{prefix}_objectives_{timestamp}.csv", objectives, delimiter=',')
        
        # Save standard errors if available
        if self.iteration_data[0]['std_errors'] is not None:
            std_errors_history = np.array([entry['std_errors'] for entry in self.iteration_data])
            np.savetxt(f"{prefix}_std_errors_{timestamp}.csv", std_errors_history, delimiter=',')
    
    def plot_convergence(self):
        """Plot objective value convergence over iterations"""
        if not self.iteration_data:
            print("No data to plot")
            return
            
        objectives = [entry['objective_value'] for entry in self.iteration_data]
        times = [entry['time_taken'] for entry in self.iteration_data]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        ax1.plot(objectives, 'o-')
        ax1.set_xlabel('Iteration')
        ax1.set_ylabel('Objective Value')
        ax1.set_title('Optimization Convergence')
        ax1.grid(True)
        
        ax2.plot(np.cumsum(times), objectives, 'o-')
        ax2.set_xlabel('Cumulative Time (seconds)')
        ax2.set_ylabel('Objective Value')
        ax2.set_title('Convergence vs Time')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.show()
    
    def plot_parameter_evolution(self):
        """Plot evolution of parameters over iterations"""
        if not self.iteration_data or not self.param_names:
            print("No parameter names set or no data to plot")
            return
            
        plt.figure(figsize=(12, 8))
        for i, name in enumerate(self.param_names):
            if i >= self.k: # Ensure we don't go out of bounds if param_names is longer than k
                break
            values = [entry['sigma_v'][i] for entry in self.iteration_data]
            plt.plot(values, 'o-', label=name)
        
        plt.xlabel('Iteration')
        plt.ylabel('Parameter Value')
        plt.title(f'Evolution of {self.k} Parameters')
        plt.legend()
        plt.grid(True)
        plt.show()
    
    def analyze_residuals(self):
        """Analyze and plot residuals across all iterations"""
        if not self.iteration_data:
            print("No data to analyze")
            return
            
        residuals = np.array([entry['residuals'] for entry in self.iteration_data])
        mean_residuals = residuals.mean(axis=0)
        std_residuals = residuals.std(axis=0)
        
        plt.figure(figsize=(12, 6))
        plt.errorbar(range(len(mean_residuals)), mean_residuals, yerr=std_residuals, 
                    fmt='o', alpha=0.5)
        plt.axhline(0, color='red', linestyle='--')
        plt.xlabel('Observation')
        plt.ylabel('Residual')
        plt.title('Residual Analysis Across Iterations')
        plt.grid(True)
        plt.show()

# =============================================
# 2. Main Optimization Routine
# =============================================

def run_optimization(X_rand_m, X_d_p, X_d_p_m, v_it, p, J, T, N, Z, delta_guess, 
                     df, max_optimisation_time=3600, inst_type=3, k=None):
    """
    Complete optimization routine with enhanced tracking
    
    Parameters:
    -----------
    X_rand_m, X_d_p_m, v_it, p, J, T, N : Model matrices and parameters
    Z : Instrument matrix
    delta_guess : Initial guess for delta
    df : DataFrame containing data
    max_optimisation_time : Maximum time to run optimization (seconds)
    inst_type : Instrument type
    n_1_1 : Number of linear parameters to track
    """
    
    # Initialize logger
    logger = OptimizationLogger(k=k)
    logger.set_parameter_names(['sigma_v'+str(i) for i in range(k)])  # Adjust based on your sigma_v size
    logger.set_X_d_p_names(list(X_d_p.columns))
    
    # Initialize variables
    total_optimization_time = 0
     # Size of sigma_v - adjust based on your model
    bnds = tuple((0, np.inf) for _ in range(k))
    
    options = {
        'disp': True,
        'maxiter': 10000,
        'maxfev': 20000,
        'xatol': 1e-6,
        'fatol': 1e-4
    }
    
    # Main optimization loop
    iteration = 0
    
    while total_optimization_time < max_optimisation_time:
        iteration += 1
        print(f"\n=== Starting Iteration {iteration} ===")
        iteration_start = time.time()
        
        try:
            # =================================
            # 1. Generate random starting point
            # =================================
            #sigma_v = np.random.uniform(size=k)
            sigma_v=np.array([0.06205418 ,2.83930727 ,2.66303685, 0. ,0.27082857])
            #sigma_v=np.array( [0.42152828 0.90740715 0.80703076 0.83142054 0.28752144])
            #second stage optimised aprameterssigma_v=np.array([3.41295472e+01 ,2.64262501e+00 ,1.20634258e-01 ,2.52001901e-03, 1.11686396e+01])
            print(f"Random starting point: {sigma_v}")
            
            # =================================
            # 2. First stage estimation
            # =================================
            print("\n--- First Stage ---")
            w1 = np.linalg.inv(Z.T @ Z)
            
            # First stage optimization
            t1 = time.time()
            res_init_wt = minimize(
                objective_1,
                sigma_v,
                args=(df.P_share.values, X_rand_m, X_d_p_m, v_it, p,
                     J, T, N, 1e-4, delta_guess, Z, w1),
                method="L-BFGS-B",
                options=options,
                bounds=bnds
            )
            
            param_2 = res_init_wt.x
            sigma_v_2 = param_2[0:k]
            print(f"First stage parameters: {sigma_v_2}")
            print(f"First stage objective: {res_init_wt.fun}")
            
            # Calculate delta and linear parameters
            delta_2 = solve_delta_1(df.P_share.values, X_rand_m, v_it, p, 
                                   delta_guess, sigma_v_2, J, T, N, 1e-5)
            b = np.linalg.inv(X_d_p_m.T @ Z @ w1 @ Z.T @ X_d_p_m) @ (X_d_p_m.T @ Z @ w1 @ Z.T @ delta_2)
            xi_w = delta_2 - X_d_p_m @ b
            
            # Plot stage 1 residuals
            plt.figure(figsize=(8, k))
            plt.scatter(X_d_p_m @ b, xi_w, alpha=0.6)
            plt.title(f"Iteration {iteration} - Stage 1 Residuals")
            plt.xlabel("Predicted Utility")
            plt.ylabel("Residuals")
            plt.grid(True)
            plt.show()
            
            # =================================
            # 3. Optimal weighting matrix
            # =================================
            obs = np.sum(J)
            g_ind = xi_w.reshape((-1,1)) * Z
            vg = (g_ind.T @ g_ind / obs)
            weight = np.linalg.inv(vg)
            
            # =================================
            # 4. Second stage estimation
            # =================================
            print("\n--- Second Stage ---")
            res = minimize(
                objective_1,
                sigma_v_2,
                args=(df.P_share.values, X_rand_m, X_d_p_m, v_it, p,
                     J, T, N, 1e-4, delta_2, Z, weight),
                method="L-BFGS-B",
                options=options,
                bounds=bnds
            )
            
            param_3 = res.x
            sigma_v_3 = param_3[0:k]
            min_obj = res.fun
            print(f"Second stage parameters: {sigma_v_3}")
            print(f"Second stage objective: {min_obj}")
            
            # Final delta and parameters
            delta_3 = solve_delta_1(df.P_share.values, X_rand_m, v_it, p, 
                                    delta_2, sigma_v_3, J, T, N, 1e-5)
            b_updated = np.linalg.inv(X_d_p_m.T @ Z @ weight @ Z.T @ X_d_p_m) @ \
                       (X_d_p_m.T @ Z @ weight @ Z.T @ delta_3)
            xi_updated = delta_3 - X_d_p_m @ b_updated
            
            # Plot stage 2 residuals
            plt.figure(figsize=(8, k))
            plt.scatter(X_d_p_m @ b_updated, xi_updated, alpha=0.6)
            plt.title(f"Iteration {iteration} - Stage 2 Residuals")
            plt.xlabel("Predicted Utility")
            plt.ylabel("Residuals")
            plt.grid(True)
            plt.show()
            
            # =================================
            # 5. Standard errors
            # =================================
            # moment = xi_updated.reshape((-1,1)) * Z
            # mean_moment = moment.mean(axis=0)
            # v1 = moment.T @ moment / moment.shape[0]
            # v2 = mean_moment * mean_moment
            # S = v1 - v2
            
            # V = np.linalg.inv(X_d_p_m.T @ Z @ weight @ Z.T @ X_d_p_m) @ \
            #     (X_d_p_m.T @ Z @ weight @ S @ weight @ Z.T @ X_d_p_m) @ \
            #     np.linalg.inv(X_d_p_m.T @ Z @ weight @ Z.T @ X_d_p_m)
            # lin_std_errors = np.sqrt(np.diag(V))
            
           # =================================
            # 5. Standard errors using full Jacobian
           # =================================

            # # Define a closure objective function for Jacobian estimation
            # def moment_closure(sigma_v_val):
            #     delta_tmp = solve_delta_1(df.P_share.values, X_rand_m, v_it, p,
            #                               delta_3, sigma_v_val, J, T, N, 1e-5)
            #     xi_tmp = delta_tmp - X_d_p_m @ b_updated
            #     moment_tmp = xi_tmp.reshape((-1, 1)) * Z
            #     return moment_tmp  # shape: (n_obs, n_instr)

            def gmm_moments(sigma_v_3):
             delta_tmp = solve_delta_1(df.P_share.values, X_rand_m, v_it, p,delta_3, sigma_v_3, J, T, N, 1e-5)
             xi_tmp = delta_tmp - X_d_p_m @ b_updated
             moment_tmp = xi_tmp.reshape((-1, 1)) * Z
             return moment_tmp  # shape: (n_obs, n_instr)


            # stderr_all, cov_all = compute_full_gmm_std_errors(
            #     sigma_v_est=sigma_v_3,
            #     beta_est=b_updated,
            #     objective_fn=moment_closure,
            #     xi_updated=xi_updated,
            #     Z=Z,
            #     X_d_p_m=X_d_p_m,
            #     weight=weight,
                # spacing=1e-5)

            stderr_all, _, _ = Gmm_std_error(sigma_v_3, b_updated, gmm_moments, xi_updated, Z, X_d_p_m, weight)
            lin_std_errors = stderr_all[k:]


            # =================================
            # 6. Log results
            # =================================
            iteration_time = time.time() - iteration_start
            total_optimization_time += iteration_time
            assert len(param_3) == k, f"Expected {k} parameters, got {len(param_3)}"
            logger.log_iteration(
                iteration=iteration,
                sigma_v=param_3,
                linear_params=b_updated,
                objective_value=min_obj,
                residuals=xi_updated,
                time_taken=iteration_time,
                std_errors=lin_std_errors,
                nonlinear_std_errors=stderr_all[:k],
                delta=delta_3,
                b_first_stage=b
            )
            
            # Save progress every 5 iterations
            if iteration % 5 == 0:
                logger.save_progress() 
            # Print summary
            print(f"\nIteration {iteration} completed in {iteration_time:.2f} seconds")
            print(f"Total optimization time: {total_optimization_time:.2f}/{max_optimisation_time} seconds")
            print(f"Current best objective: {logger.best_objective}")
            
        except Exception as e:
            print(f"\nError in iteration {iteration}: {str(e)}")
            logger.save_progress(filename_prefix='error_dump')
            raise
    
    # Final save and plots
    logger.save_progress()
    logger.plot_convergence()
    logger.plot_parameter_evolution()
    logger.analyze_residuals()
    
    return logger










import statsmodels.api as sm
def two_stage_least_squares(y, X_d_p_m, Z):
    """
    y      : Dependent variable (n x 1 vector)
    X_d_p_m  : Endogenous regressor matrix (n x k matrix)
    Z      : Instrument matrix (n x m matrix)

    Returns:
  
    Beta_2sls : Estimated coefficients (k x 1 vector)
    """
    # First stage: Regress X_d_m on Z to get the fitted values
    # Projection matrix P_Z = Z * (Z'Z)^(-1) * Z'
    P_Z = Z @ np.linalg.inv(Z.T @ Z) @ Z.T  # Projection onto the instrument space

    # Fitted values from the first stage
    X_hat = P_Z @ X_d_p_m  # Predicted values of X_d_m

    # Second stage: Regress y on the fitted values (X_hat)
    Beta_2sls = np.linalg.inv(X_hat.T @ X_hat) @ X_hat.T @ y  # 2SLS estimator

    return Beta_2sls


    #2sls using stast model:
def two_stage_using_api(y, X_d_p_m, Z):
    """
    y      : Dependent variable (n x 1 vector)
    X_d_m  : Endogenous regressor matrix (n x k matrix)
    Z      : Instrument
    X_with_intercept = sm.add_constant(data['Z'])  # add intercept to the instrument
    """
    first_stage_model = sm.OLS(X_d_p_m, Z).fit()

  # Obtain fitted values from the first stage
    fitted_X = first_stage_model.fittedvalues

  # Second stage regression: Regress Y on fitted X values
  #X_with_intercept_fitted = sm.add_constant(fitted_X)  # add intercept to the fitted values
    second_stage_model = sm.OLS(y, fitted_X).fit()

    return second_stage_model

def create_interaction_columns(df, tech_dummies, brand_dummies, tech_levels, brands):
    """
    Adds interaction columns between tech levels and brand dummies to the DataFrame.

    Parameters:
    -----------
    df : pd.DataFrame
        Main DataFrame to which new interaction columns will be added.
    tech_dummies : pd.DataFrame
        DataFrame containing dummy variables for each tech level (e.g., OS version).
    brand_dummies : pd.DataFrame
        DataFrame containing dummy variables for each brand.
    tech_levels : list
        List of tech level values to create interactions for (e.g., [4.0, 4.5, 5.0]).
    brands : list
        List of brand names (columns in brand_dummies) to include in interactions.

    Returns:
    --------
    interaction_cols : list
        List of names of the newly created interaction columns.
    """

    interaction_cols = []

    for tech in tech_levels:
        for brand in brands:
            col_name = f'{tech}_x_{brand}'
            df[col_name] = tech_dummies[tech] * brand_dummies[brand]
            interaction_cols.append(col_name)

    return interaction_cols

    

def svd_analysis(A, k=None, plots=False):
    """
    Perform SVD on the input matrix A and return analysis results.
    
    Parameters:
    A : numpy.ndarray
        The input matrix to perform SVD on (m x n).
    k : int, optional
        The number of components to retain for dimensionality reduction. If None, full SVD analysis is performed.
        
    Returns:
    dict : A dictionary containing:
        - 'U': Left singular vectors
        - 'Sigma': Singular values
        - 'VT': Right singular vectors
        - 'explained_variance': Variance explained by each singular value
        - 'cumulative_variance': Cumulative explained variance
        - 'A_reduced': Reduced matrix if k is provided, else None
    """
    # Perform Singular Value Decomposition (SVD)
    U, Sigma, VT = np.linalg.svd(A, full_matrices=False)
    
    # Explained variance and cumulative explained variance
    explained_variance = (Sigma**2) / np.sum(Sigma**2)
    cumulative_variance = np.cumsum(explained_variance)
    
    # If k is provided, reduce the matrix to k components
    A_reduced = None
    if k is not None:
        U_k = U[:, :k]
        Sigma_k = np.diag(Sigma[:k])
        VT_k = VT[:k, :]
        A_reduced = np.dot(U_k, np.dot(Sigma_k, VT_k))
    if plots:
      # Plot the singular values
      plt.figure(figsize=(8, 6))
      plt.plot(Sigma, marker='o', linestyle='--', color='b')
      plt.title('Singular Values')
      plt.xlabel('Index')
      plt.ylabel('Singular Value')
      plt.grid(True)
      plt.show()

      # Plot the cumulative explained variance
      plt.figure(figsize=(8, 6))
      plt.plot(cumulative_variance, marker='x', linestyle='-', color='g')
      plt.title('Cumulative Explained Variance')
      plt.xlabel('Number of Components')
      plt.ylabel('Cumulative Variance Explained')
      plt.grid(True)
      plt.show()

    # Return a dictionary with results
    return {
        'U': U,
        'Sigma': Sigma,
        'VT': VT,
        'explained_variance': explained_variance,
        'cumulative_variance': cumulative_variance,
        'reduced': A_reduced
        }


#The following function may be completely wrong , tread carefully

def var_cov_matrix(sigma_v, xi_updated,delta, s ,X_rand, X_d_p_m,p,Z,J,T,N,tol,W):
  """sigma_v: are optimla non linear estimates
  Xi_updated are updated resdiuals, delta_old are starting values of delta, s is share which is used to cmopute delta
  z: instrument matrix
  x_rand: matrix of X which has non linear paraemters
  W is teh optimal weighing matrix




  """
  k2=sigma_v.shape[0] #numbber of non linear parameteers
  # Z_p=z #instruments
  xi=xi_updated

  #demand instruments
  Z_cols=Z.shape[1] # number of columns in Z

  #moment conditions ( interaction of all residuals wioth individual instruments)

  M=np.repeat(xi[:,np.newaxis],Z_cols,axis=1)* Z

  V=M.T@ M

  h=1e-8
  H = np.eye(k2) * h
  k=[]

    # If k2 > 1, calculate the remaining components
  for i in range(k2):
     h_plus=solve_delta(s,X_rand,v,p, delta,sigma_v+ H[i],J,T,N,tol)
     h_minus=(solve_delta(s,X_rand,v,p, delta,sigma_v - H[i],J,T,N,tol))
     K.append((h_plus-h_minus)/(2*h))

  K=np.column_stack(K)

  Gamma=Z.T @np.hstack([-X_d_p_m,K])
  B = Gamma.T @ W @ Gamma
  Omega = Gamma.T @ W @ V @ W @ Gamma

  f=np.linalg.inv(B) @ Omega @ np.linalg.inv(B)

  return f

        

def quality_para(theta_q_start, x_q_m):
    """
    This function takes the values from theta_q_start, which are the 2SLS perturbed estimates, and returns the estimated coefficients.
    The reference value (in this case "screen size") is excluded from the returned coefficients, as it is normalized to 1.

    Args:
        theta_q_start (array-like): The initial parameter estimates from 2SLS, where the first element is the intercept.
        x_q (array-like): The set of quality characteristics (e.g., screen size, processor speed, etc.), where the first variable is the reference.

    Returns:
        quality_coeff (array-like): The estimated coefficients for the quality variables, excluding the reference (screen size).
    """
    K1 = x_q_m.shape[1]  # Length of quality characteristics,
    
    # Extract quality coefficients starting from index 1, as the first variable is the the reference variable
    quality_coeff = theta_q_start[1:K1]  
    
    # K2 = len(x_rand) + 1  # This seems to be an unused part, so I will comment it out for now.
    # random_dim = range(K1 + 1, K1 + K2)  # This was also commented in the original code, but seems related to some random part of the model
    # radnom_part = sigma_v_q(random_dim)  # Not used in the final output
    
    return quality_coeff


# Here x is the characteristics that have random coefficients
@jit(nopython=False)
def util_iter_q(out, X_rand, v, p, delta, sigma_v, J, T, N):
    """This utility function has a q subscript in the end and takes quality as a measure rather than individual characteristics.
    Quality has features that enter quality: Screen size, Processor speed, Storage, Camera.

    In the utility function below, we could also start with sigma_v values coming from a log-normal distribution
    and then simply take the exponential of "out" in the function. I think intuitively this should work.

    1) X_rand should be nothing but a vector of [ price, cons].  quality is  added at the beginning of the vector later:
    """
    # D is the demographic variables vector. D*1 column vector.
    # sigma_v and sigma_b are random coefficients of the demographic factors
    # First iterate over the individuals
    for i in prange(N):  # prange just means that this loop will process in parallel.
        # iterator through t and J
        tj = 0
        for t in prange(T):
            # Market size of market t
            mktSize = J[t]  # J is the total number of goods in market t.

            # V is the unobserved demographic variables which are random normal
            # sigma_v is the variable associated with unobserved variable v following normal distribution.
            # Think of sigma_v as the variance of V.

            # Iterate over goods in a particular market
            # V is the part of utility which is individual dependent

            # Please note that theta is numerically a column vector. But in Python, we do not really need to mention
            # it as a row or column if the vector is 1D. Python will internally manipulate it.
            # We have 5 characteristics in X and hence k=5

            # X_rand = [quality, price, cons] --- Quality is a function of Screen size, Storage, Processor speed, and Camera
            #quality will be added at the end of the vector
            #WE KEEP QUALITY at the beginning of the utILITY FUNCTION. THIS IS IMPORTANT SINCE WE WILL ADD QUALITY at the beginnig of  X_rand later
            for j in prange(mktSize):
                out[tj, i] = delta[tj] + \
                    X_rand[tj, 0] * v[N * t + i, 0] * sigma_v[0] -\
                    X_rand[tj,1] * v[N * t + i, 1] * sigma_v[1]+ \
                    X_rand[tj, 2] * v[N * t + i, 2] * sigma_v[2]
                    # Uncomment if you need to include additional variables
                    # v[N * t + i, 1] * theta_2[1] * x[tj, 1] +
                    # v[N * t + i, 2] * theta_2[2] * x[tj, 2] + \
                    # v[N * t + i, 3] * theta_2[3] * x[tj, 3] + \
                    # v[N * t + i, 4] * theta_2[4] * x[tj, 4] - \
                    # v[N * t + i, 5] * theta_2[5] * p[tj] + \
                    # d_it[N * t + i, 0] * sigma_b[0] * x[tj, 0] + \
                    # d_it[N * t + i, 1] * sigma_b[1] * x[tj, 1] + \
                    # d_it[N * t + i, 2] * sigma_b[2] * x[tj, 2] + \
                    # d_it[N * t + i, 3] * sigma_b[3] * x[tj, 3] + \
                    # d_it[N * t + i, 0] * sigma_b[4] * x[tj, 4] - \
                    # d_it[N * t + i, 0] * sigma_b[5] * p[tj]

                tj += 1

    return out


@jit(nopython=False)
def compute_indirect_utility_q(X_rand, v, p, delta, sigma_v, J, T, N):
    """Compute the indirect utility for each individual in each market.

    Args:
        X_rand: Product characteristics (observed and unobserved).
        v: Random coefficients for individual-specific effects.
        p: Price vector.
        delta: Vector of constants (market-specific effects).
        sigma_v: Variances for random coefficients.
        J: Number of products in each market.
        T: Number of markets.
        N: Number of individuals.

    Returns:
        out: Computed utility values for all individuals and products.
    """
    # Ensure sigma_v values are positive
    sigma_v = np.abs(sigma_v)

    # Output matrix
    out = np.zeros((sum(J), N))  # Initial guess of Out, which then keeps updating in the iteration

    # Call the iteration function to calculate utilities
    # out = util_iter(out, x, v, p, y, delta, theta_2, J, T, N)
    out = util_iter_q(out, X_rand, v, p, delta, sigma_v, J, T, N)

    return out

@jit(nopython=False)
def compute_share_q(x_rand, v, p,delta, sigma_v, J, T, N):
    q = np.zeros((np.sum(J), N))

    # obtain vector of indirect utilities
    u =compute_indirect_utility_q(x_rand, v, p,delta, sigma_v, J, T, N)
    #because we have such huge values of our utilities, it is better to subtract the value of utility wight hemax value of utility across  goods for each indicidual.
    #this will not blow up the exponents. Teh folowing line takes the max of each column.
    #max_u=np.max(u,axis=0,keepdims=True) #axis 0 signifies that we run dwon along the rows. so, there will be max of each column.
    # exponentiate the utilities
    # print("indirect_utility is ",u)
    # print("-"*40)
    # print("Shape: ",u.shape)
    # print("max of utility =", np.max(u))
    # print("min of utiltiy",np.min(u))
    exp_u = np.exp(u)
    # print("exponential of utility is:",np.max(exp_u))

    # pointer to first good in the market
    first_good = 0

    for t in range(T):
        # market size of market t
        mktSize = J[t]

        # calculate the numerator of the share eq
        numer = exp_u[first_good:first_good + mktSize,:]
        # print("numer is : ",numer)
        # calculate the denom of the share eq
        #denom= 1+np.exp(logsumexp(u,axis=0))
        denom = 1 + numer.sum(axis = 0)
        #print("denom is : ",denom)


        # calculate the quantity each indv purchases of each good in each market
        q[first_good:first_good + mktSize,:] = numer/denom #q is equal to s_ijt and we take the average for the the number
        #of conumsers to get s_jt which is mentioned below as s
        #print("num/deno: ", numer/denom)
        first_good += mktSize

    # to obtain shares, assume that each simulation carries the same weight.
    # this averages each row, which is essentially computing the sahres for each
    #  good in each market.
    s = q @ (np.repeat(1/N, N)) #This simply is dividing the q computed previously with N. this gives us share.
    # print("max value of share",np.max(s))
    # print("min value of share",np.min(s))
    return (q,s) # q therefore gives us each individula quanitty and s is solving the integral through monetcarlo simulation whihcis nothing but equal to
    #taking average

#delta is solved through contraction mapping. We will later use this as an inner loop in our objective functrion.
#This function uses delta as a class object. For a simpler and direct use , check the funciton below: solve_delta_1

@jit(nopython=False)
def solve_delta_1_q(s, x_rand, v, p, delta, sigma_v, J, T, N, tol):
    # Define the tolerance variable
    eps = 10

    # Initialize delta_old as the current delta array
    delta_old = delta.copy()

    while eps > tol:
        # Aviv's step 1: obtain predicted shares and quantities
        q_s = compute_share_q(x_rand, v, p, delta_old, sigma_v, J, T, N)
        sigma_jt = q_s[1]  # Extract the shares
        delta_new = delta_old + np.log(s / sigma_jt)  # Update delta using contraction mapping
        #print("compute share: ",q_s)
        # Update the tolerance and check for convergence
        eps = np.max(np.abs(delta_new - delta_old))

        # Update delta_old for the next iteration
        delta_old = delta_new.copy()

    # Return the updated delta
    return delta_old

def objective_1_q(sigma_v_q, theta_q_start,  s, X_rand, X_q_m, X_d_p_m, v, p, J, T, N, tol, delta,
                  Z, weigh):
    """
    This version is taking a quality vector instead of characteristics. 
    The quality is a function of x_q (q = x_q * theta_q) 
    and the starting values of theta_q are nothing but the perturbed 2SLS estimates.

    The changes will be made to the following variables:
    1) sigma_v : This will now have not 6 but 3 starting values since random coefficients are now three 
       [Quality, price, cons] instead of 6 from before. We won't make any changes to the original function, 
       but we will just add a new variable sigma_v_q which will have starting values of the random coefficients.
    2) Theta_q_start: starting values of theta_q.
    3) X_d_p_q: Instead of X_d_p, we will now use X_d_p_q.
    4) v will now be a matrix with v with dimensions equal to X_rand_q.
    5) X_rand will change to X_rand_q.
    """
    
    # Optimum function will flatten the parameters, for us to be able to use sigma_d as a matrix, we need to reshape it like this.
    # sigma_v = param[0:6]
    
    
    # Restricting parameters to 0
    obs = np.sum(J)

    # Aviv's step 1 & 2:
    # delta_1 = solve_delta_1(s, X_d_p_m, v, p, delta, sigma_v, J, T, N, tol)  # Solves value for delta for a given guess of theta_2 (the nonlinear parameters)
    delta_1 = solve_delta_1_q(s, X_rand, v, p, delta, sigma_v_q, J, T, N, tol)
    
    # Obtain the actual implied quantities and shares from converged delta
    # q_s = compute_share(x, v, d_it, p, d.delta, sigma_v, sigma_d, J, T, N)  # For given value of delta, compute share
    
    # Calculate marginal costs
    d2 = delta_1.reshape((-1, 1))  # Reshaping delta_1 for further calculations
    
    # Create the matrix for instruments
    z = Z
    
    # Debugging section (uncomment if needed)
    # pdb.set_trace()
    
    # Calculating the GMM objective function
    b = np.linalg.inv(X_d_p_m.T @ z @ weigh @ z.T @ X_d_p_m) @ (X_d_p_m.T @ z @ weigh @ z.T @ d2)
    
    # Get the error term (xi, also called omega)
    xi_w = d2 - X_d_p_m @ b
    g = z.T @ xi_w
    
    # Objective function
    obj = g.T @ weigh @ g
    
    # Check for NaN or inf values in the objective function
    if np.isnan(obj) or np.isinf(obj):
        obj = 1e20
    
    print([sigma_v_q, obj])
    
    return obj
def adding_quality(theta_q_start,X_rand_q_m,X_q_m,X_d_p_m):
    theta_q_start = quality_para(theta_q_start, X_q_m)  # This only takes the parameters which are part of q, excluding the reference variable
    theta_q_start = np.insert(theta_q_start, 0, 1)
    
    # Compute the quality matrix
    quality = np.dot(X_q_m,theta_q_start).reshape((-1,1))   # The negative sign is for taking the last values instead of the first.

    # Dimensions of X_d when we account for quality
    dim_x_d = X_d_p_m.shape[1] - X_q_m.shape[1]
    
    X_d_p_m = np.hstack((quality,X_d_p_m[:, -dim_x_d:]))
    #print("X_d_p_m shape=",X_d_p_m.shape)
    X_rand_q_m=np.hstack((quality,X_rand_q_m))
    print("X_rand shape:",X_rand_q_m.shape)
    return X_rand_q_m,X_d_p_m
ite=0
def callback(x):
    """
    This callback function will be called at each iteration of the optimization.
    It tracks the updated X_rand and X_d_p_m matrices.
    """
    
    global ite

    ite += 1 
    # Assuming you pass `X_rand` and `X_d_p_m` into the function as args, you need to update them.
    # In the original function, X_rand and X_d_p_m are modified in-place, so we simply save them here.
    # You can print or log them if needed
    # print(f"Updated X_rand: {X_rand_updated}")
    # print(f"Updated X_d_p_m: {X_d_p_m_updated}")
    print("iteration:",ite)

# Initialize variables to track updated X_rand and X_d_p_m
# X_rand_updated = None
# X_d_p_m_updated = None
# def callback(x, X_rand, X_d_p_m):
#     """
#     This callback function will be called at each iteration of the optimization.
#     It tracks the updated X_rand and X_d_p_m matrices.
#     """
    
#     global X_rand_updated , X_d_p_m_updated, ite

#     ite += 1 
#     # Assuming you pass `X_rand` and `X_d_p_m` into the function as args, you need to update them.
#     # In the original function, X_rand and X_d_p_m are modified in-place, so we simply save them here.
#     X_rand_updated = X_rand
#     X_d_p_m_updated = X_d_p_m
#     # You can print or log them if needed
#     # print(f"Updated X_rand: {X_rand_updated}")
#     # print(f"Updated X_d_p_m: {X_d_p_m_updated}")
#     print("iteration:",ite)













