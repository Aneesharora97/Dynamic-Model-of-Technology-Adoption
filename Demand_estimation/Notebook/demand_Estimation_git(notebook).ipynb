{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook estimates BLP demand side model by optimising GMM objective function to generate demand side estimates. Refer here for theoretical underpinnings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# Import Required Libraries\n",
    "# =======================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "from scipy import stats, special\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from numba import jit, njit, prange\n",
    "import pyblp\n",
    "import importlib\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "# Optional installs (if not already installed)\n",
    "# !pip install miceforest\n",
    "# !pip install pyblp\n",
    "# !pip install linearmodels\n",
    "\n",
    "\n",
    "# Display full DataFrame when inspecting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# File Paths (Update if needed)\n",
    "# =======================================\n",
    "phone_path = \"/content/Phonearena_data(Feb,2024).csv\"\n",
    "cpi_path = \"/content/CPI data ( 2009-2023).csv\"\n",
    "final_data_path = \"/content/FINAL DATA_1.csv\"\n",
    "function_path = \"/content/myfunctions_Dec_31_24.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# ðŸ”§ 3. Import Custom Functions\n",
    "# =======================================\n",
    "sys.path.append('/content')\n",
    "from myfunctions_Dec_31_24 import *\n",
    "import myfunctions_Dec_31_24\n",
    "importlib.reload(myfunctions_Dec_31_24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "#  Load Main Dataset\n",
    "# =======================================\n",
    "\n",
    "df = pd.read_csv(final_data_path)\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# =======================================\n",
    "#  Basic Cleaning and Sorting\n",
    "# =======================================\n",
    "df.sort_values(by='Quarter', inplace=True)\n",
    "\n",
    "# Remove duplicate rows based on product characteristics within a quarter\n",
    "df.drop_duplicates(\n",
    "    subset=[\"Quarter\", \"Brand\", \"Screen Size\", \"Storage (GB)\", \"RAM (GB)\", \"Model Name\"],\n",
    "    keep='first',\n",
    "    inplace=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# Generate Unique Identifiers\n",
    "# =======================================\n",
    "\n",
    "# Assign a model ID to every row (for models that may repeat with different specs)\n",
    "df.insert(9, 'Model_Id', [f'ID_{i+1}' for i in range(len(df))])\n",
    "\n",
    "# Factorize brand name to assign unique brand IDs\n",
    "df['Brand_Id'] = pd.factorize(df['Brand'])[0]\n",
    "\n",
    "# Optional: Dictionary to look up brand name from brand ID\n",
    "brand_id_lookup = dict(enumerate(df['Brand'].unique()))\n",
    "\n",
    "# Define key characteristics (to be used later in analysis)\n",
    "characteristics_columns = ['Screen Size', 'RAM (GB)', 'weight(g)', 'Processor Speed Band', 'Price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# Extract Weight from Text\n",
    "# =======================================\n",
    "def extract_grams(weight_string):\n",
    "    \"\"\"Extract numerical gram value from string.\"\"\"\n",
    "    if isinstance(weight_string, str):\n",
    "        match = re.search(r\"(\\d+\\.\\d+)\\s*g\", weight_string)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "    return np.nan\n",
    "\n",
    "df['weight(g)'] = df['Weight:'].apply(extract_grams)\n",
    "\n",
    "\n",
    "\n",
    "# =======================================\n",
    "#  Fill Missing Weights Manually\n",
    "# =======================================\n",
    "# Weights manually collected from reliable sources. \n",
    "\n",
    "smartphone_weights_grams = {\n",
    "    \"SAMSUNGGALAXYS\":116,\n",
    "    'MOTOROLABRAVO': 122.0,\n",
    "    'T-MOBILEMYTOUCH': 138.0,\n",
    "    'T-MOBILECOMET': 115.0,\n",
    "    'MOTOROLADROIDX': 155.0,\n",
    "    'LGVORTEX': 139.0,\n",
    "    'LGOPTIMUSM': 128.0,\n",
    "    'LGOPTIMUSS': 130.0,\n",
    "    'HUAWEIASCEND': 130.0,\n",
    "    'HTCHD2': 157.0,\n",
    "    'HTCEVO': 170.0,\n",
    "    'HTCINSPIRE4G': 164.0,\n",
    "    'MOTOROLADROIDX2': 158.0,\n",
    "    'SAMSUNGDROIDCHARGE': 142.88,\n",
    "    'LGOPTIMUX2X': 147.0,\n",
    "    'HUAWEIM835': 110.0,\n",
    "    'HTCEVO3D': 170.0,\n",
    "    'AT&TIMPULSE': 125.0,\n",
    "    'LGTHRILL': 168.0,\n",
    "    'ZTESCORE': 125.0,\n",
    "    'SAMSUNGGALAXYS2SKYROCKET': 132.0,\n",
    "    'SAMSUNGGALAXYNEXUS': 135.0,\n",
    "    'SAMSUNGFOCUSFLASH': 116.2,\n",
    "    'HTCHEROS': 120.0,\n",
    "    'SAMSUNGGALAXYATTAIN': 132.0,\n",
    "    'SAMSUNGGALAXYSBLAZE': 128.0,\n",
    "    'SAMSUNGFOCUS2': 122.0,\n",
    "    'SAMSUNGGALAXYSAVIATOR': 144.0,\n",
    "    'LGLUCID': 127.0,\n",
    "    'MOTOROLADROIDRAZRM': 126.0,\n",
    "    'T-MOBILEMYTOUCHQ': 134.0,\n",
    "    'SAMSUNGGALAXYSLIGHTRAY': 142.0,\n",
    "    'SAMSUNGREVERB': 127.5,\n",
    "    'LGSPLENDO': 147.0,\n",
    "    'KYOCERARISE': 130.0,\n",
    "    'SAMSUNGGALAXYY': 97.5,\n",
    "    'ZTEANTHEM': 138.0,\n",
    "    'LGSPECTRUM2': 145.0,\n",
    "    'HUAWEIASCENDY': 120.0,\n",
    "    'HUAWEIFUSION2': 120.0,\n",
    "    'HUAWEIASCENDP1': 130.0,\n",
    "    'SAMSUNGATIVODYSSEY': 125.0,\n",
    "    'LGSPIRIT': 118.0,\n",
    "    'PANTECHPERCEPTION': 140.0,\n",
    "    'ZTEVITAL': 150.0,\n",
    "    'ZTEIMPERIAL': 159.0,\n",
    "    'LGOPTIMUSGPRO': 172.0,\n",
    "    'SAMSUNGGALAXYDISCOVER': 122.0,\n",
    "    'ZTEOPEN': 117.8,\n",
    "    'LGOPTIMUSF6': 122.0,\n",
    "    'ALCATELONETOUCHSCRIBE': 135.0,\n",
    "    'LGOPTIMUSEXTREME': 130.0,\n",
    "    'HUAWEIVITRIA': 146.0,\n",
    "    'ZTESOURCE': 136.5,\n",
    "    'ALCATELONETOUCHEVOLVE': 136.5,\n",
    "    'LGOPTIMUSDYNAMIC': 124.7,\n",
    "    'LGOPTIMUSEXCEED2': 135.0,\n",
    "    'ZTEVALET': 136.5,\n",
    "    'COOLPADFLO': 140.0,\n",
    "    'LGGVISTA': 173.0,\n",
    "    'MOTOROLAMOTOE': 142.0,\n",
    "    'LGTRIBUTE': 139.0,\n",
    "    'ZTEWARPSYNC': 145.0,\n",
    "    'COOLPADQUATTROII': 141.5,\n",
    "    'LGF60': 127.0,\n",
    "    'ZTESPEED': 155.0,\n",
    "    'ZTEGRANDXMAXPLUS': 170.0,\n",
    "    'LGLEON': 140.0,\n",
    "    'BLUSTUDIO6.0HD': 190.0,\n",
    "    'ALCATELONETOUCHPOPSTAR2': 147.0,\n",
    "    'ASUSZENFONE2': 272.0, # This weight is as per previous data, consider if it's correct for a phone.\n",
    "    'HUAWEISNAPTO': 140.0,\n",
    "    'BLUSTUDIO5.0CHD': 160.0,\n",
    "    'ZTESOLAR': 155.0,\n",
    "    'COOLPADROGUE': 130.0,\n",
    "    'HTCDESIRE626S': 140.0,\n",
    "    'BLUVIVOSELFIE': 150.0,\n",
    "    'LGGVISTA2': 165.0,\n",
    "    'ALCATELONETOUCHFIERCEXL': 173.0,\n",
    "    'BLUDASHX': 148.0,\n",
    "    'BLULIFEONEX(2016)': 141.0,\n",
    "    'BLUSTUDIOGPLUS': 170.0,\n",
    "    'LGTRIBUTE5': 143.0,\n",
    "    'BLUVIVOXL': 152.0,\n",
    "    'BLUSTUDIOGHD': 130.0,\n",
    "    'BLUDASHXPLUS': 190.0,\n",
    "    'SAMSUNGGALAXYJ3V': 152.0,\n",
    "    'BLUDASHL': 122.0,\n",
    "    'ALCATELONETOUCHPIXIECLIPSE': 295.0, # As noted before, this seems like a tablet weight.\n",
    "    'BLUDASHM': 157.0,\n",
    "    'BLUNEOXPLUS': 227.0, # This still seems high for a phone, but kept from source.\n",
    "    'BLUADVANCE5.0': 140.6,\n",
    "    'COOLPADCATALYST': 139.0,\n",
    "    'MOTOROLAMOTOZ': 136.0,\n",
    "    'ALCATELONETOUCHPIXI4(5)': 168.0,\n",
    "    'BLUR1HD': 144.0,\n",
    "    'ALCATELONETOUCHPOP4PLUS': 156.0,\n",
    "    'HUAWEISENSA': 155.0,\n",
    "    'NOKIA6': 169.0,\n",
    "    'SAMSUNGGALAXYJ3PRIME': 148.0,\n",
    "    'ZTEMAJESTYPRO': 140.0,\n",
    "    'ZTEBLADEXMAX': 180.0,\n",
    "    'ZTEMAXXL': 176.0,\n",
    "    'ZTEBLADEMAX3': 176.0,\n",
    "    'SAMSUNGGALAXYJ7PERX': 167.0,\n",
    "    'SAMSUNGGALAXYS8ACTIVE': 208.0,\n",
    "    'HTCDESIRE555': 160.0,\n",
    "    'NOKIA2': 153.0,\n",
    "    'ZTEAVID4': 160.0,\n",
    "    'ZTETEMPOGO': 140.0,\n",
    "    'HONOR7X': 165.0,\n",
    "    'LGSTYLO4': 172.0,\n",
    "    'LGK30': 160.0,\n",
    "    'SAMSUNGGALAXYJ3(2018)': 152.0,\n",
    "    'BLUDASHL5': 148.0,\n",
    "    'LGK8(2018)': 152.0,\n",
    "    'MOTOROLAMOTOE5': 174.0,\n",
    "    'ALCATELTETRA': 150.0,\n",
    "    'BLUVIVOX': 165.0,\n",
    "    'SAMSUNGGALAXYSOL3': 133.0,\n",
    "    'ZTEVISIBLER2': 150.0,\n",
    "    'BLUPUREVIEW': 155.0,\n",
    "    'BLUC5': 146.0,\n",
    "    'AT&TAXIA': 748.0, # This is still likely the package weight. Specific phone weight not found.\n",
    "    'COOLPADLEGACY': 180.0,\n",
    "    'LGESCAPEPLUS': 170.0,\n",
    "    'COOLPADLEGACYS': 180.0,\n",
    "    'LGNEONPLUS': 170.0,\n",
    "    'LGSTYLO6': 219.0,\n",
    "    'LGXPRESSIONPLUS3': 152.0,\n",
    "    'ONEPLUS8T': 188.0,\n",
    "    'TCLA3': 160.0,\n",
    "    'ZTEBLADE11PRIME': 186.0,\n",
    "    'TCL20S': 190.0\n",
    "}\n",
    "\n",
    "\n",
    "# Apply mapping for models with missing weights\n",
    "df['weight(g)'] = df['weight(g)'].fillna(df['Model Name'].map(smartphone_weights_grams))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# CPI Adjustment for Real Prices\n",
    "# =======================================\n",
    "cpi = pd.read_csv(cpi_path)\n",
    "\n",
    "# Merge CPI by year into main dataset\n",
    "df = pd.merge(df, cpi, on=\"Year\", how='left')\n",
    "\n",
    "# Normalize CPI to 2011 base\n",
    "cpi_base = df.loc[df['Year'] == 2011, 'CPI'].values[0]\n",
    "df['CPI_index'] = df['CPI'] / cpi_base\n",
    "\n",
    "# Create inflation-adjusted price columns\n",
    "df['Adj_prices'] = df['ASP'] * df['CPI_index']\n",
    "df['Price'] = df['Adj_prices'] / 1000  # Price scaled in $1000s\n",
    "\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# Normalize Exchange Rates\n",
    "# =======================================\n",
    "normalising_exchange(df, ['Japan', 'Korea', 'China'])\n",
    "\n",
    "# =======================================\n",
    "# Merge PhoneArena Dataset (RAM, Camera, Processor)\n",
    "# =======================================\n",
    "phone_arena = pd.read_csv(phone_path)[['Model Name', 'RAM:', 'Processor:', 'Main camera:']]\n",
    "merged = pd.merge(df, phone_arena, on='Model Name', how='left')\n",
    "\n",
    "# =======================================\n",
    "# Fill Missing Features (RAM, Camera, Processor)\n",
    "# =======================================\n",
    "# RAM\n",
    "df['Ram'] = df['RAM (GB)'].fillna(df['RAM:']).fillna(merged['RAM:_y'])\n",
    "df['Ram'] = df['Ram'].astype(str).str.extract(r'(\\d+\\.\\d+|\\d+)').astype(float)\n",
    "\n",
    "# Camera\n",
    "df['Camera'] = df['Main camera:'].fillna(df['\\n\\nCamera\\n\\n']).fillna(merged['Main camera:_y'])\n",
    "df['Camera'] = df['Camera'].astype(str).str.extract(r'(\\d+\\.\\d+|\\d+)').astype(float)\n",
    "\n",
    "# Fix known errors\n",
    "df.loc[df['Model Name'].isin(['BLACKBERRY8820', 'BLACKBERRY8830']), 'Camera'] = 0\n",
    "df.loc[df['Model Name'] == \"MOTOROLAMOTOE(2015)\", 'Model Name'] = \"MOTOROLAMOTOE(2NDGEN)\"\n",
    "df.loc[df['Model Name'] == \"SAMSUNGGALAXYNEXUS\", \"Camera\"] = 5\n",
    "\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# Extract Processor Speed in GHz\n",
    "# =======================================\n",
    "# Fix band values like \"08: 2.8GHz+\" â†’ \"3GHz\"\n",
    "df['Processor Speed Band'] = df['Processor Speed Band'].replace({'08: 2.8GHz+': '3GHz'})\n",
    "\n",
    "# Extract GHz from Processor Speed Band\n",
    "df['Processor Speed (Band)'] = df['Processor Speed Band'].str.extract(r'([\\d\\.]+)').astype(float)\n",
    "\n",
    "# Extract MHz from Processor description\n",
    "mhz = df['Processor:'].str.split(',', expand=True)[1]\n",
    "df['Processor Speed (Raw)'] = mhz.str.extract(r'([\\d\\.]+)').astype(float) / 1000\n",
    "\n",
    "# Fill processor speed from raw, then fallback to band\n",
    "df['Processor Speed'] = df['Processor Speed (Raw)'].fillna(df['Processor Speed (Band)'])\n",
    "\n",
    "# =======================================\n",
    "#  Compute smartphone Model Age\n",
    "# =======================================\n",
    "df['Quarter_q'] = df['Quarter'].copy()\n",
    "df['Quarter'] = pd.to_datetime(df['Quarter'])\n",
    "\n",
    "# First appearance of each model\n",
    "first_seen = df.groupby('Model Name')['Quarter'].min()\n",
    "df['first_appeared'] = df['Model Name'].map(first_seen)\n",
    "df['first_appeared'] = pd.to_datetime(df['first_appeared'])\n",
    "\n",
    "# Age in quarters\n",
    "df['Age'] = ((df['Quarter'].dt.year - df['first_appeared'].dt.year) * 4 +\n",
    "             (df['Quarter'].dt.quarter - df['first_appeared'].dt.quarter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =======================================\n",
    "#  Log-Transforms and Scaling\n",
    "# =======================================\n",
    "df['Storage (GB)'] = df['Storage (GB)'] / 1000\n",
    "df['Screen Size'] = df['Screen Size'] / 10\n",
    "df['Ram'] = df['Ram'] / 10\n",
    "df['Camera'] = df['Camera'] / 100\n",
    "df['Processor Speed'] = df['Processor Speed'] / 10\n",
    "df['weight(g)'] = df['weight(g)'] / 100\n",
    "\n",
    "# Recompute log characteristics\n",
    "char_cols = ['Screen Size', 'Camera', 'Processor Speed', 'weight(g)', 'Price']\n",
    "ln_cols = [f'ln_{col}' for col in char_cols]\n",
    "df[ln_cols + ['ln_Age', 'ln_Spec_tech']] = np.log(df[char_cols + ['Age', 'Spec_tech']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# Filter for Estimation Sample\n",
    "# =======================================\n",
    "# Remove units with low sales\n",
    "df = df[df['Units'] > 5000]\n",
    "\n",
    "# Drop rows with missing/zero values in key characteristics\n",
    "key_cols = [\"Screen Size\", \"Storage (GB)\", \"Camera\", \"Processor Speed\", \"Age\", \"Spec_tech\"]\n",
    "df = df.dropna(subset=key_cols)\n",
    "df = df[(df[\"Storage (GB)\"] > 0) & (df[\"Camera\"] > 0) & (df[\"Processor Speed\"] > 0) & (df[\"Screen Size\"] > 0)]\n",
    "\n",
    "# Filter out pre-2011\n",
    "df = df[df['Year'] != 2010]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =======================================\n",
    "# Variables for GMM Estimation\n",
    "# =======================================\n",
    "df['Market size'] = 200_000_000\n",
    "df['P_share'] = df['Units'] / df['Market size']\n",
    "df['Outside_share'] = 1 - df.groupby('Quarter')['P_share'].transform(\"sum\")\n",
    "df['diff2'] = np.log(df['P_share']) - np.log(df['Outside_share'])\n",
    "df['ln_price'] = np.log(df[\"Price\"])\n",
    "df['Cons'] = 1\n",
    "\n",
    "# Time trend (used for Spec_tech parameter)\n",
    "quarter_map = {q: i+1 for i, q in enumerate(df['Quarter'].unique())}\n",
    "df['Time Trend'] = df['Quarter'].map(quarter_map)\n",
    "\n",
    "# =======================================\n",
    "# Instrument Construction & Dropping\n",
    "# =======================================\n",
    "instrument_cols = generate_instruments(df, [\"Cons\", \"Screen Size\", \"Storage (GB)\", 'Camera', \"Processor Speed\", \"weight(g)\"])\n",
    "df = df.dropna(subset=instrument_cols + ['Japan', 'Korea', 'China'])\n",
    "df = df.loc[~((df['Camera'] == 0) | (df['Storage (GB)'] == 0))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# Save Clean Version \n",
    "# =======================================\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final columns to retain\n",
    "final_cols = [\n",
    "    'Quarter', 'Quarter_q', 'Year', 'Brand', 'Model Name', 'Model_Id', 'Brand_Id', 'OS Version',\n",
    "    'Screen Size', 'Storage (GB)', 'weight(g)', 'Ram', 'Processor Speed', 'Age', 'Camera',\n",
    "    'Spec_tech', 'Price', 'Units', 'ln_Screen Size', 'ln_Camera', 'ln_Processor Speed', 'ln_Price',\n",
    "    'ln_Age', 'ln_Spec_tech', 'Japan', 'Korea', 'China', 'Japan_n', 'Korea_n', 'China_n',\n",
    "    'first_appeared', 'ln_weight(g)'\n",
    "]\n",
    "df = df[final_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Constructing Dummies#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# Dummies\n",
    "# =======================================\n",
    "\n",
    "#storage dummies\n",
    "\n",
    "storage_bins = [0, 1, 4, 16, 64, 128, 256, 512, 1024, 2048, 5120]\n",
    "df['storage_bin'] = pd.cut(df['Storage (GB)'], bins=storage_bins, labels=False)\n",
    "storage_dummies = pd.get_dummies(df['storage_bin'], prefix=\"Storage\", dtype=float, drop_first=True)\n",
    "\n",
    "# Mahalanobis and Isolation Forest outliers\n",
    "df['Outlier'] = get_isolation_forest_outliers(df, characteristics_columns_p)\n",
    "df['mahal'] = get_mahalanobis_distances(df, characteristics_columns_p)\n",
    "\n",
    "# Thresholding\n",
    "threshold = np.sqrt(chi2.ppf(0.997, df=5))  # 5 variables included\n",
    "df = df[(df['Outlier'] == 1) & (df['mahal'] <= threshold)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# --- DUMMY VARIABLES ---\n",
    "# =======================================\n",
    "\n",
    "specific_brands = ['Apple', 'LG', 'Motorola', 'Samsung']\n",
    "df['Brand_d'] = df['Brand'].apply(lambda x: x if x in specific_brands else 'Other')\n",
    "brand_dummies = pd.get_dummies(df['Brand_d'], dtype=int).drop('Other', axis=1)\n",
    "\n",
    "tech_dummies = pd.get_dummies(df['Spec_tech'], dtype=int)\n",
    "tech_dummies = tech_dummies.drop([2.0, 2.5, 3.0], axis=1)\n",
    "\n",
    "df['Model Name_d'] = df['Model Name'].apply(lambda x: x if df.loc[df['Model Name'] == x, 'Brand'].values[0] in specific_brands else 'Other')\n",
    "model_dummies = pd.get_dummies(df['Model Name_d'], dtype=int).drop('Other', axis=1)\n",
    "\n",
    "to_drop_q = [\"2017-01-01\", \"2017-04-01\", \"2017-07-01\", \"2017-10-01\"]\n",
    "quarter_dummies = pd.get_dummies(df['Quarter'], dtype=int).drop(to_drop_q, axis=1)\n",
    "\n",
    "time_trend_m = df['Time Trend'].to_numpy().reshape(-1, 1)\n",
    "tech_technology_m = tech_dummies.to_numpy() * time_trend_m\n",
    "\n",
    "df['Brand_quarter'] = df['Brand_d'] + '-' + df['Quarter'].astype(str)\n",
    "brand_quarter_dummies = pd.get_dummies(df['Brand_quarter'], dtype=int)\n",
    "brand_quarter_dummies = brand_quarter_dummies.loc[:, ~brand_quarter_dummies.columns.str.contains('Other')]\n",
    "brand_quarter_dummies = brand_quarter_dummies.drop(['Apple-2021-04-01', 'LG-2021-04-01', 'Motorola-2014-04-01', 'Samsung-2011-01-01'], axis=1)\n",
    "\n",
    "df['Brand_Year'] = df['Brand_d'] + '-' + df['Year'].astype(str)\n",
    "brand_year_dummies = pd.get_dummies(df['Brand_Year'], dtype=int).drop('Samsung-2020', axis=1)\n",
    "\n",
    "year_dummies = pd.get_dummies(df['Year'], drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "#  INTERACTIONS & FINAL DATA MATRICES ---\n",
    "# =======================================\n",
    "\n",
    "\n",
    "df['Age_square'] = df['Age'] ** 2\n",
    "delta_0 = df['diff2'].values\n",
    "J = df.groupby('Quarter')['Model Name'].count().values\n",
    "T = len(J)\n",
    "N = 500  # Simulated individuals per market\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =======================================\n",
    "\t# Scale characteristics\n",
    "# =======================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df[characteristics_columns_p + ['Age']])\n",
    "df_scaled = df.copy()\n",
    "df_scaled[characteristics_columns_p + ['Age']] = X_scaled\n",
    "df_scaled['Cons'] = 1\n",
    "\n",
    "X_q = df[q_cols].to_numpy()\n",
    "X_rand_p = df_scaled[[\"Screen Size\", \"Camera\", 'weight(g)', \"Cons\", \"Price\"]]\n",
    "X_rand = df_scaled[[\"Screen Size\", \"Camera\", 'weight(g)', \"Cons\"]]\n",
    "X_rand_q = df_scaled[[\"Price\", \"Cons\"]]\n",
    "\n",
    "X_d = df_scaled[[\"Cons\", \"Screen Size\", \"Camera\", 'weight(g)', \"Age\"]]\n",
    "X_d = pd.concat([X_d, tech_dummies, quarter_dummies, brand_dummies, brand_quarter_dummies], axis=1)\n",
    "\n",
    "X_d_p = df_scaled[[\"Cons\", \"Screen Size\", \"Camera\", 'weight(g)', \"Age\", \"Price\"]]\n",
    "X_d_p = pd.concat([X_d_p, tech_dummies, quarter_dummies, brand_dummies, brand_quarter_dummies], axis=1)\n",
    "\n",
    "X_d_m = X_d.to_numpy()\n",
    "X_d_p_m = X_d_p.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =======================================\n",
    "# \t\tInstrument matrix\n",
    "# =======================================\n",
    "\n",
    "inst_cols = [\"Screen Size\", \"Storage (GB)\", \"weight(g)\", 'Camera', 'Processor Speed']\n",
    "exch_cols = ['Japan_n', 'Korea_n', 'China_n']\n",
    "\n",
    "df_pyblp_inst = df[inst_cols + ['Quarter_q', \"Brand_Id\"]]\n",
    "rename_dict = {'Screen Size': 'Screen', 'Storage (GB)': \"Storage\", 'Camera': 'Camera', 'Processor Speed': 'Processor',\n",
    "               'Brand_Id': 'firm_ids', \"Quarter_q\": \"market_ids\", \"weight(g)\": \"weight\"}\n",
    "formulation = 'Screen + Storage + Camera + weight + Processor'\n",
    "\n",
    "pyblp_inst = prepare_pyblp_instruments(df_pyblp_inst, inst_cols, rename_dict, formulation)[1]\n",
    "pyblp_inst_diff = prepare_pyblp_instruments(df_pyblp_inst, inst_cols, rename_dict, formulation)[2]\n",
    "\n",
    "Z_p = df[instr_cols + exch_cols]\n",
    "Z_p_m = Z_p.to_numpy()\n",
    "exch_inst_m = df[exch_cols].to_numpy()\n",
    "Z_fan_yang = fan_yang_inst(df_scaled, inst_cols).to_numpy()\n",
    "Z_fan_yang_1 = fan_yang_inst_1(df_scaled, inst_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# \t\tVIF check and drop\n",
    "# =======================================\n",
    "\n",
    "vif_Z = compute_vif(Z_fan_yang_1)\n",
    "high_vif_cols = vif_Z[vif_Z['VIF'] > 10]['Variable'].tolist()\n",
    "Z_fan_yang_1 = Z_fan_yang_1.drop(high_vif_cols, axis=1)\n",
    "Z_fan_yang_1_m = Z_fan_yang_1.to_numpy()\n",
    "\n",
    "# IV matrix setup\n",
    "X_iv = pd.concat([df[[\"Screen Size\", \"Camera\", 'weight(g)', \"Age\"]], tech_dummies, quarter_dummies, brand_dummies, brand_quarter_dummies], axis=1)\n",
    "X_iv_m = X_iv.to_numpy()\n",
    "\n",
    "\n",
    "# =======================================\n",
    "#     Final instrument sets\n",
    "# =======================================\n",
    "\n",
    "Z_non_pyblp = np.hstack((X_iv_m, Z_p_m))\n",
    "Z_pyblp = np.hstack((X_iv_m, pyblp_inst))\n",
    "Z_pyblp_diff = np.hstack((X_iv_m, pyblp_inst_diff))\n",
    "Z_fan_yang_m = np.hstack((X_iv_m, Z_fan_yang))\n",
    "Z_fan_yang_1_m = np.hstack((X_iv_m, Z_fan_yang_1_m))\n",
    "\n",
    "Z_pyblp_ex = np.hstack((Z_pyblp, exch_inst_m))\n",
    "Z_pyblp_diff_ex = np.hstack((Z_pyblp_diff, exch_inst_m))\n",
    "Z_fan_yang_ex = np.hstack((Z_fan_yang_m, exch_inst_m))\n",
    "Z_fan_yang_1_ex = np.hstack((Z_fan_yang_1_m, exch_inst_m))\n",
    "\n",
    "# =======================================\n",
    "#  \t\tInstrument selector\n",
    "# =======================================\n",
    "\n",
    "inst_map = {\n",
    "    0: Z_pyblp,\n",
    "    1: Z_pyblp_diff,\n",
    "    2: Z_fan_yang_m,\n",
    "    3: Z_fan_yang_1_m,\n",
    "    4: Z_non_pyblp,\n",
    "    5: Z_pyblp_ex,\n",
    "    6: Z_pyblp_diff_ex,\n",
    "    7: Z_fan_yang_ex,\n",
    "    8: Z_fan_yang_1_ex\n",
    "}\n",
    "inst_type = 3\n",
    "Z = inst_map.get(inst_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# \t\t\tDiagnostics\n",
    "# =======================================\n",
    "\n",
    "cond_dict = {}\n",
    "for key, matrix in inst_map.items():\n",
    "    cond_dict[key] = {\n",
    "        \"full_rank\": np.linalg.matrix_rank(matrix) == matrix.shape[1],\n",
    "        \"condition_number\": np.linalg.cond(matrix)\n",
    "    }\n",
    "# =======================================\n",
    "# \tQR decomposition for multicollinearity analysis\n",
    "# =======================================\n",
    "\n",
    "Q, R, piv = qr(Z_fan_yang_1_m, mode='economic', pivoting=True)\n",
    "independent_cols = sorted(piv[:np.linalg.matrix_rank(Z_fan_yang_1_m)])\n",
    "dependent_cols = sorted(set(range(Z_fan_yang_1_m.shape[1])) - set(independent_cols))\n",
    "\n",
    "Z_df = pd.concat([X_iv, Z_fan_yang_1], axis=1)\n",
    "independent_names = Z_df.columns[independent_cols]\n",
    "dependent_names = Z_df.columns[dependent_cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Some Preliminary analyses/Results#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Descriptive stats and summaries ---\n",
    "columns_describe = ['Units'] + characteristics_columns\n",
    "describe = df.groupby(['Brand', 'Quarter', \"Model Name\"])[columns_describe].sum().reset_index()\n",
    "describe['Units'] /= 1000\n",
    "describe = describe.sort_values('Quarter')\n",
    "\n",
    "# ---------------------------------------\n",
    "# OLS Regression: Price ~ Instruments\n",
    "# ---------------------------------------\n",
    "print(\"-\" * 60)\n",
    "print(\"OLS regression of Price on Instruments (Z_fan_yang_1)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Fit OLS model\n",
    "ols_price_iv = sm.OLS(p, Z_fan_yang_1).fit()\n",
    "\n",
    "# Print adjusted R-squared\n",
    "print(\"Adjusted R-squared:\", ols_price_iv.rsquared_adj)\n",
    "\n",
    "# Print full summary\n",
    "print(ols_price_iv.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "quarterly_results = describe.groupby('Quarter')[columns_describe].mean().reset_index()\n",
    "stats = quarterly_results.describe().loc[['mean', 'min', 'max', 'std']].transpose()\n",
    "print(stats)\n",
    "print(describe.shape)\n",
    "\n",
    "# --- OLS Regressions ---\n",
    "df['const'] = 1\n",
    "x = pd.concat([df[characteristics_columns + ['Age', 'Price', 'Time Trend']], tech_dummies, Brand_dummies], axis=1)\n",
    "full_model_ols = sm.OLS(df['diff2'], x).fit()\n",
    "\n",
    "print(\"OLS with Trend, Tech, Brand Dummies\")\n",
    "print(summary_col([full_model_ols], stars=True))\n",
    "\n",
    "# --- OLS with interaction terms ---\n",
    "interaction_cols = create_interaction_columns(df, tech_dummies=tech_dummies,\n",
    "                                              brand_dummies=Brand_dummies,\n",
    "                                              tech_levels=[4.0, 4.5, 5.0],\n",
    "                                              brands=specific_brands)\n",
    "x_int = pd.concat([\n",
    "    df[characteristics_columns + ['Age', 'Price', 'Time Trend'] + interaction_cols],\n",
    "    tech_dummies,\n",
    "    Brand_dummies\n",
    "], axis=1)\n",
    "full_model_int_ols = sm.OLS(df['diff2'], x_int).fit()\n",
    "print(full_model_int_ols.summary())\n",
    "\n",
    "# --- 2SLS regressions ---\n",
    "y = df['diff2']\n",
    "x = pd.concat([X_iv, X_d_p[['Age', 'Price']], Year_dummies], axis=1)\n",
    "Z_ls = pd.concat([x, Z_fan_yang], axis=1)\n",
    "result = two_stage_using_api(y, x, Z_ls)\n",
    "print(\"2SLS Coefficients:\", result.params)\n",
    "print(\"2SLS Std Errors:\", result.bse)\n",
    "print(result.summary())\n",
    "\n",
    "# --- 2SLS using IV2SLS ---\n",
    "X_endog = X_d_p['Price']\n",
    "W_exog = pd.concat([Brand_dummies, tech_dummies, Year_dummies, df[characteristics_columns + ['Age']]], axis=1)\n",
    "Z_instr = Z_fan_yang_1\n",
    "iv_model = IV2SLS(dependent=y, exog=W_exog, endog=X_endog, instruments=Z_instr).fit()\n",
    "print(iv_model.summary)\n",
    "print(\"Sargan stat:\", iv_model.sargan)\n",
    "\n",
    "# --- 2SLS with interaction terms ---\n",
    "W_exog_int = pd.concat([\n",
    "    Brand_dummies,\n",
    "    tech_dummies,\n",
    "    df[characteristics_columns + ['Age', 'Time Trend'] + interaction_cols]\n",
    "], axis=1)\n",
    "iv_model_int = IV2SLS(y, W_exog_int, endog=X_endog, instruments=Z_instr).fit(cov_type='kernel')\n",
    "print(iv_model_int.summary)\n",
    "\n",
    "# --- IV Logit First Stage Diagnostics ---\n",
    "X_tilde = np.hstack((X_d_m, p.reshape(-1, 1)))\n",
    "zxw1 = Z.T @ X_d_p_m\n",
    "\n",
    "if np.linalg.matrix_rank(Z) < Z.shape[1]:\n",
    "    raise ValueError(\"Multicollinearity in instrument matrix Z\")\n",
    "if np.linalg.matrix_rank(X_d_p_m) < X_d_p_m.shape[1]:\n",
    "    raise ValueError(\"Multicollinearity in characteristics matrix X_d_p_m\")\n",
    "\n",
    "bx1 = np.linalg.inv(zxw1.T @ zxw1) @ zxw1.T @ Z.T @ delta_0\n",
    "e = delta_0 - (X_d_p_m @ bx1)\n",
    "g_ind = e.reshape((-1, 1)) * Z\n",
    "demean = g_ind - g_ind.mean(axis=0).reshape((1, -1))\n",
    "vg = demean.T @ demean / demean.shape[0]\n",
    "w0 = np.linalg.inv(vg)\n",
    "cc_1 = np.linalg.inv(zxw1.T @ w0 @ zxw1) @ zxw1.T @ w0 @ Z.T @ delta_0\n",
    "\n",
    "print(\"Initial weighting matrix stats:\", w0.max(), w0.min())\n",
    "print(cc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Initial Guess and GMM Objective ---\n",
    "sigma_v = np.random.normal(0, 1, size=k)\n",
    "t0 = time.time()\n",
    "delta_guess = df.diff2.values\n",
    "\n",
    "if not np.allclose(delta_guess, d.delta):\n",
    "    raise ValueError(\"Delta guess mismatch\")\n",
    "\n",
    "obj = objective_1(param=sigma_v,\n",
    "                  shares=df.P_share.values,\n",
    "                  X_rand=X_rand_m,\n",
    "                  X_d=X_d_m,\n",
    "                  v_it=v_it,\n",
    "                  prices=p,\n",
    "                  J=J, T=T, N=N,\n",
    "                  tol=1e-3,\n",
    "                  delta=delta_guess,\n",
    "                  Z=Z,\n",
    "                  W=w0)\n",
    "print(\"GMM Objective Initial Value:\", obj)\n",
    "print(\"Computation Time:\", time.time() - t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Sanity Checks and Quick Diagnostics\n",
    "# ---------------------------\n",
    "\n",
    "print(\"X_iv shape:\", X_iv.shape)\n",
    "print(\"Z_fan_yang_1 shape:\", Z_fan_yang_1.shape)\n",
    "print(\"Correlation matrix of Z:\")\n",
    "display(pd.DataFrame(Z).corr())\n",
    "print(\"Eigenvalues of Z.T @ Z:\")\n",
    "print(np.linalg.eigvals(Z.T @ Z))\n",
    "print(\"Initial delta_0:\", delta_0)\n",
    "\n",
    "# Inspect key matrices\n",
    "print(\"Head of X_rand:\")\n",
    "display(X_rand.head())\n",
    "print(\"Summary of X_rand:\")\n",
    "print(X_rand.describe())\n",
    "print(\"Models with Camera >= 5.3:\")\n",
    "display(df[df_scaled['Camera'] >= 5.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Indirect Utility Computation (Validation Run)\n",
    "# ---------------------------\n",
    "\n",
    "_ = compute_indirect_utility(X_rand_p_m, v_it, p, df.diff2.values, sigma_v, J, T, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Reload Custom Functions\n",
    "# ---------------------------\n",
    "\n",
    "import importlib\n",
    "import myfunctions_Dec_31_24\n",
    "importlib.reload(myfunctions_Dec_31_24)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Optimization Setup\n",
    "# ---------------------------\n",
    "\n",
    "max_time = 60 * 60   # Max optimization time (1 hour)\n",
    "inst_type = 8        # Fan & Yang 1 instruments + exchange rates\n",
    "n_1_1 = 19           # Number of linear parameters to track\n",
    "\n",
    "# Run the GMM optimization\n",
    "logger = run_optimization(\n",
    "    X_rand_m=X_rand_m,\n",
    "    X_d_p=X_d_p,\n",
    "    X_d_p_m=X_d_p_m,\n",
    "    v_it=v_it,\n",
    "    p=p,\n",
    "    J=J,\n",
    "    T=T,\n",
    "    N=N,\n",
    "    Z=Z,\n",
    "    delta_guess=df.diff2.values,\n",
    "    df=df,\n",
    "    max_optimisation_time=max_time,\n",
    "    inst_type=inst_type,\n",
    "    k=k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Best Results Summary\n",
    "# ---------------------------\n",
    "\n",
    "print(\"\\n=== Best GMM Solution Found ===\")\n",
    "print(f\"Objective value: {logger.best_objective}\")\n",
    "\n",
    "print(\"\\nNon-linear parameters (Ïƒ_v):\")\n",
    "for name, value in zip(logger.param_names, logger.best_params['sigma_v']):\n",
    "    print(f\"{name}: {value:.6f}\")\n",
    "\n",
    "print(\"\\nLinear parameters (Î²):\")\n",
    "print(logger.best_params['linear_params'])\n",
    "\n",
    "print(\"\\nStandard errors:\")\n",
    "print(logger.best_params['std_errors'])\n",
    "\n",
    "# Get long format parameter DataFrame\n",
    "param_df = logger.get_linear_param_long_format()\n",
    "\n",
    "# Inspect price coefficient over iterations\n",
    "display(param_df[param_df['parameter'] == \"Price\"])\n",
    "\n",
    "# Timestamp of completion\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "print(\"Eastern Time:\", datetime.now(ZoneInfo(\"America/New_York\")))\n",
    "\n",
    "# Get iteration with minimum objective value\n",
    "min_obj_dict = min(logger.iteration_data, key=lambda x: x['objective_value'])\n",
    "min_iter = min_obj_dict['iteration']\n",
    "\n",
    "best_df = param_df[param_df['iteration'] == min_iter]\n",
    "print(\"Best parameter estimates from iteration\", min_iter)\n",
    "display(best_df)\n",
    "\n",
    "print(\"Best Ïƒ_v:\", min_obj_dict['sigma_v'])\n",
    "print(\"Std errors from best iteration:\", min_obj_dict['std_errors'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# GMM Moments and Weight Matrix Recalculation\n",
    "# ---------------------------\n",
    "\n",
    "def gmm_moments(sigma_v_3):\n",
    "    delta_tmp = solve_delta_1(df.P_share.values, X_rand_m, v_it, p, delta_check, sigma_v_check, J, T, N, 1e-5)\n",
    "    xi_tmp = delta_tmp - X_d_p_m @ b_check\n",
    "    moment_tmp = xi_tmp.reshape((-1, 1)) * Z\n",
    "    return moment_tmp\n",
    "\n",
    "b_check = logger.best_params['linear_params']\n",
    "sigma_v_check = logger.best_params['sigma_v']\n",
    "delta_check = logger.best_params['delta']\n",
    "obs_check = Z.shape[0]\n",
    "\n",
    "# Recompute moments and weight matrix\n",
    "xi_check = delta_check - X_d_p_m @ b_check\n",
    "g_check = xi_check.reshape((-1, 1)) * Z\n",
    "vg_check = (g_check.T @ g_check) / obs_check\n",
    "weight_check = np.linalg.inv(vg_check)\n",
    "\n",
    "# GMM robust standard errors\n",
    "_ = gmm_moments(sigma_v_check)\n",
    "Gmm_std_error(sigma_v_check, b_check, gmm_moments, xi_check, Z, X_d_p_m, weight_check)\n",
    "\n",
    "# Final export\n",
    "param_df.to_csv(\"/content/param_df(06_23_2025).csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
